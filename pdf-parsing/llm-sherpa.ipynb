{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://llmsherpa.readthedocs.io/en/latest/llmsherpa.readers.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmsherpa.readers import LayoutPDFReader\n",
    "from llmsherpa.readers import LayoutReader\n",
    "from llmsherpa.readers.layout_reader import ListItem, Paragraph, Table\n",
    "from pprint import pprint\n",
    "from IPython.display import display, HTML\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmsherpa_api_url = \"http://localhost:5010/api/parseDocument?renderFormat=all&applyOcr=yes&useNewIndentParser=yes\"\n",
    "pdf_url = \"https://arxiv.org/pdf/1910.13461.pdf\" # also allowed is a file path e.g. /home/downloads/xyz.pdf\n",
    "pdf_file = \"documents/word.pdf\"\n",
    "pdf_reader = LayoutPDFReader(llmsherpa_api_url)\n",
    "doc_url = pdf_reader.read_pdf(pdf_url)\n",
    "doc_file = pdf_reader.read_pdf(pdf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n"
     ]
    }
   ],
   "source": [
    "print(doc_url.sections()[0].to_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
      "{mikelewis,yinhanliu,naman}@fb.com\n",
      "Abstract\n",
      "1 Introduction\n",
      "B D A B C D E\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(doc_url.sections()[i].to_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n",
      "\n",
      "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n",
      "BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n",
      "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n",
      "We evaluate a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the original sentences and using a novel in-ﬁlling scheme, where spans of text are replaced with a single mask token.\n",
      "BART is particularly effective when ﬁne tuned for text generation but also works well for comprehension tasks.\n",
      "It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n",
      "BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n",
      "We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most inﬂuence end-task performance.\n",
      "\n",
      "methods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n",
      "The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n",
      "Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n",
      "However, these methods typically focus on particular types of end tasks (e.g. span prediction, generation, etc.), limiting their applicability.\n",
      "\n",
      "In this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n",
      "BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n",
      "Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n",
      "BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n",
      "\n",
      "A key advantage of this setup is the noising ﬂexibility; arbitrary transformations can be applied to the original text, including changing its length.\n",
      "We evaluate a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the original sentences and using a novel in-ﬁlling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n",
      "This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(doc_url.chunks()[i].to_text(), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table><th><td colSpan=1>Title of Each Class</td><td colSpan=1>Trading Symbol(s)</td><td colSpan=1>Name of Each Exchange on Which Registered</td></th><tr><td colSpan=1>Common Stock, par value $.01 per share</td><td colSpan=1>AMZN</td><td colSpan=1>Nasdaq Global Select Market</td></tr><tr><td>Securities registered pursuant to Section 12(g) of the Act: None</td></tr><tr><td colSpan=1>Indicate by check mark if the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act.</td><td colSpan=1>Yes ☒</td><td colSpan=1>No ☐</td></tr><tr><td colSpan=1>Indicate by check mark if the registrant is not required to file reports pursuant to Section 13 or Section 15(d) of the Exchange Act.</td><td colSpan=1>Yes ☐</td><td colSpan=1>No ☒</td></tr><tr><td>Indicate by check mark whether the registrant (1) has filed all reports required to be filed by Section 13 or 15(d) of the Securities Exchange Act of 1934 during the preceding 12 months (or for such shorter period that the registrant was required to file such reports), and (2) has been subject to such filing requirements for the past 90 days. Yes ☒ No ☐</td></tr><tr><td>Indicate by check mark whether the registrant has submitted electronically every Interactive Data File required to be submitted pursuant to Rule 405 of Regulation S-T during the preceding 12 months (or for such shorter period that the registrant was required to submit such files). Yes ☒ No ☐</td></tr><tr><td>Indicate by check mark whether the registrant is a large accelerated filer, an accelerated filer, a non-accelerated filer, a smaller reporting company, or an emerging growth company. See the definitions of “large accelerated filer,” “accelerated filer,” “smaller reporting company,” and “emerging growth company” in Rule 12b-2 of the Exchange Act.</td></tr><tr><td colSpan=1>Large accelerated filer ☒</td><td colSpan=1></td><td colSpan=1>Accelerated filer ☐</td></tr><tr><td colSpan=1>Non-accelerated filer ☐</td><td colSpan=1></td><td colSpan=1>Smaller reporting company ☐</td></tr></table>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_file.tables()[0].to_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><th><td colSpan=1>Title of Each Class</td><td colSpan=1>Trading Symbol(s)</td><td colSpan=1>Name of Each Exchange on Which Registered</td></th><tr><td colSpan=1>Common Stock, par value $.01 per share</td><td colSpan=1>AMZN</td><td colSpan=1>Nasdaq Global Select Market</td></tr><tr><td>Securities registered pursuant to Section 12(g) of the Act: None</td></tr><tr><td colSpan=1>Indicate by check mark if the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act.</td><td colSpan=1>Yes ☒</td><td colSpan=1>No ☐</td></tr><tr><td colSpan=1>Indicate by check mark if the registrant is not required to file reports pursuant to Section 13 or Section 15(d) of the Exchange Act.</td><td colSpan=1>Yes ☐</td><td colSpan=1>No ☒</td></tr><tr><td>Indicate by check mark whether the registrant (1) has filed all reports required to be filed by Section 13 or 15(d) of the Securities Exchange Act of 1934 during the preceding 12 months (or for such shorter period that the registrant was required to file such reports), and (2) has been subject to such filing requirements for the past 90 days. Yes ☒ No ☐</td></tr><tr><td>Indicate by check mark whether the registrant has submitted electronically every Interactive Data File required to be submitted pursuant to Rule 405 of Regulation S-T during the preceding 12 months (or for such shorter period that the registrant was required to submit such files). Yes ☒ No ☐</td></tr><tr><td>Indicate by check mark whether the registrant is a large accelerated filer, an accelerated filer, a non-accelerated filer, a smaller reporting company, or an emerging growth company. See the definitions of “large accelerated filer,” “accelerated filer,” “smaller reporting company,” and “emerging growth company” in Rule 12b-2 of the Exchange Act.</td></tr><tr><td colSpan=1>Large accelerated filer ☒</td><td colSpan=1></td><td colSpan=1>Accelerated filer ☐</td></tr><tr><td colSpan=1>Non-accelerated filer ☐</td><td colSpan=1></td><td colSpan=1>Smaller reporting company ☐</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "html_content = doc_file.tables()[0].to_html()\n",
    "display(HTML(html_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could consider splitting text by pages. But continuous text data may be lost in that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☒ ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934 or\n"
     ]
    }
   ],
   "source": [
    "print(doc_file.chunks()[58].to_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☐\n",
      "For the transition period from to.\n",
      "AMAZON.COM, INC.\n",
      "Delaware\n",
      "91-1646860 410 Terry Avenue North\n",
      "Seattle, Washington 98109-5210 (206) 266-1000 Securities registered pursuant to Section 12(b) of the Act:\n"
     ]
    }
   ],
   "source": [
    "for i in range(59, 65):\n",
    "    print(doc_file.chunks()[i].to_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We Have a Rapidly Evolving Business Model and Our Stock Price Is Highly Volatile\n",
      "Legal and Regulatory Risks\n",
      "Government Regulation Is Evolving and Unfavorable Changes Could Harm Our Business\n",
      "We Face Additional Tax Liabilities and Collection Obligations\n",
      "We Are Subject to Risks Related to Government Contracts and Related Procurement Regulations\n",
      "Item 1B. Unresolved Staff Comments\n"
     ]
    }
   ],
   "source": [
    "for i in range(59, 65):\n",
    "    print(doc_file.sections()[i].to_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the RAW Json over the defualt PDF Reader for greater customisability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = LayoutReader()\n",
    "block = test.read(doc_file.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bbox': [90.0, 72.24, 511.3, 146.39999999999998],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 0,\n",
      "  'level': 0,\n",
      "  'page_idx': 0,\n",
      "  'sentences': ['[Congressional Record Volume 170, Number 41 (Thursday, March '\n",
      "                '7, 2024)] [Senate] [Pages S2272-S2277] From the Congressional '\n",
      "                'Record Online through the Government Publishing Office '\n",
      "                '[www.gpo.gov]'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [108.0, 212.16, 492.0, 317.76],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 1,\n",
      "  'level': 0,\n",
      "  'page_idx': 0,\n",
      "  'sentences': ['PRESIDENTIAL MESSAGE REPORT ON THE STATE OF THE UNION '\n",
      "                'DELIVERED TO A JOINT SESSION OF CONGRESS ON MARCH 7, 2024--PM '\n",
      "                '41'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 336.72, 498.0, 395.52],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 2,\n",
      "  'level': 1,\n",
      "  'page_idx': 0,\n",
      "  'sentences': ['The PRESIDING OFFICER laid before the Senate the following '\n",
      "                'message from the President of the United States which was '\n",
      "                'ordered to lie on the table:'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 414.48, 456.0, 457.68],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 3,\n",
      "  'level': 1,\n",
      "  'page_idx': 0,\n",
      "  'sentences': ['To the Congress of the United States: Good evening.',\n",
      "                'Mr. Speaker.',\n",
      "                'Madam Vice President.',\n",
      "                'Members of Congress.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 461.28, 210.0, 473.28],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 4,\n",
      "  'level': 1,\n",
      "  'page_idx': 0,\n",
      "  'sentences': ['My Fellow Americans.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 476.88, 498.0, 535.44],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 5,\n",
      "  'level': 1,\n",
      "  'page_idx': 0,\n",
      "  'sentences': ['In January 1941--President Franklin Roosevelt came to this '\n",
      "                'chamber to speak to the Nation.',\n",
      "                'He said---``I address you at a moment unprecedented'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 554.64, 174.0, 566.64],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 6,\n",
      "  'level': 1,\n",
      "  'page_idx': 0,\n",
      "  'sentences': ['[[Page S2273]]'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 585.84, 498.0, 675.6],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 7,\n",
      "  'level': 1,\n",
      "  'page_idx': 0,\n",
      "  'sentences': [\"in the history of the Union\\\\\\\\\\\\'\\\\\\\\\\\\'.\",\n",
      "                'Hitler was on the march.',\n",
      "                'War was raging in Europe.',\n",
      "                \"President Roosevelt\\\\\\\\\\\\'s purpose was to wake up the \"\n",
      "                'Congress and alert the American people that this was no '\n",
      "                'ordinary moment.',\n",
      "                'Freedom and democracy were under assault in the world.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 679.2, 498.0, 753.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 8,\n",
      "  'level': 1,\n",
      "  'page_idx': 0,\n",
      "  'sentences': ['Tonight--I come to the same chamber to address the Nation.',\n",
      "                'Now it is we who face an unprecedented moment in the history '\n",
      "                'of the Union.',\n",
      "                'And yes--my purpose tonight is to both wake up this Congress '\n",
      "                'and alert the American people that this is no ordinary moment '\n",
      "                'either.',\n",
      "                'Not since President Lincoln and the Civil War have freedom '\n",
      "                'and democracy been under assault here at home as they are '\n",
      "                'today.',\n",
      "                'What makes our moment rare is that freedom and democracy are '\n",
      "                'under attack both at home and overseas at the very same '\n",
      "                'time.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 181.2, 498.0, 411.12],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 9,\n",
      "  'level': 1,\n",
      "  'page_idx': 1,\n",
      "  'sentences': ['Overseas--Putin of Russia--is on the march.',\n",
      "                'Invading Ukraine--and sowing chaos throughout Europe and '\n",
      "                'beyond.',\n",
      "                'If anybody in this room thinks Putin will stop at Ukraine--I '\n",
      "                'assure you--he will not.',\n",
      "                'But Ukraine can stop Putin--if we stand with Ukraine.',\n",
      "                'And provide the weapons it needs to defend itself.',\n",
      "                'That is all Ukraine is asking.',\n",
      "                'They are not asking for American soldiers.',\n",
      "                'In fact--there are no American soldiers at war in Ukraine.',\n",
      "                'And I am determined to keep it that way.',\n",
      "                'But now--assistance for Ukraine is being blocked by those who '\n",
      "                'want us to walk away from our leadership in the world.',\n",
      "                \"It wasn\\\\\\\\\\\\'t that long ago when a Republican \"\n",
      "                'President--Ronald Reagan--thundered--``Mr.',\n",
      "                \"Gorbachev--tear down this wall\\\\\\\\\\\\'\\\\\\\\\\\\'.\"],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 414.48, 492.0, 504.48],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 10,\n",
      "  'level': 1,\n",
      "  'page_idx': 1,\n",
      "  'sentences': ['Now--my predecessor--a former Republican President--tells '\n",
      "                \"Putin-- ``Do whatever the hell you want.\\\\\\\\\\\\'\\\\\\\\\\\\' A \"\n",
      "                'former American President actually said that--bowing down--to '\n",
      "                'a Russian leader.',\n",
      "                \"It\\\\\\\\\\\\'s outrageous.\",\n",
      "                \"It\\\\\\\\\\\\'s dangerous.\",\n",
      "                \"It\\\\\\\\\\\\'s unacceptable.\"],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 507.84, 480.0, 566.6400000000001],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 11,\n",
      "  'level': 1,\n",
      "  'page_idx': 1,\n",
      "  'sentences': ['America is a founding member of NATO--the military alliance '\n",
      "                'of democratic nations--created after World War II to prevent '\n",
      "                'war and keep the peace.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 570.24, 492.0, 628.8],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 12,\n",
      "  'level': 1,\n",
      "  'page_idx': 1,\n",
      "  'sentences': [\"Today--I\\\\\\\\\\\\'ve made NATO stronger than ever.\",\n",
      "                'We welcomed Finland to the Alliance last year.',\n",
      "                'And just this morning--Sweden officially joined NATO--and '\n",
      "                'their Prime Minister--is here tonight.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 632.4, 456.0, 675.6],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 13,\n",
      "  'level': 1,\n",
      "  'page_idx': 1,\n",
      "  'sentences': ['Mr. Prime Minister--welcome to NATO--the strongest military '\n",
      "                'alliance the world has ever known.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 679.2, 504.0, 753.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 14,\n",
      "  'level': 1,\n",
      "  'page_idx': 1,\n",
      "  'sentences': ['I say this to Congress--we must stand up to Putin.',\n",
      "                'Send me the Bipartisan National Security Bill.',\n",
      "                'History is watching.',\n",
      "                'If the United States walks away now--it will put Ukraine at '\n",
      "                'risk.',\n",
      "                'Europe at risk.',\n",
      "                'The free world at risk--emboldening others who wish to do us '\n",
      "                'harm.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 72.24, 511.3, 753.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 15,\n",
      "  'level': 1,\n",
      "  'page_idx': 2,\n",
      "  'sentences': ['My message to President Putin is simple.',\n",
      "                'We will not walk away.',\n",
      "                'We will not bow down.',\n",
      "                'I will not bow down.',\n",
      "                'History is watching.',\n",
      "                'Just like history watched 3 years ago on January 6th.',\n",
      "                'Insurrectionists stormed this very Capitol and placed a '\n",
      "                'dagger at the throat of American democracy.',\n",
      "                'Many of you were here on that darkest of days.',\n",
      "                'We all saw with our own eyes these insurrectionists were not '\n",
      "                'patriots.',\n",
      "                'They had come to stop the peaceful transfer of power and to '\n",
      "                'overturn the will of the people.',\n",
      "                'January 6th--the lies about the 2020 election--and the plots '\n",
      "                'to steal the election--posed the gravest threat to our '\n",
      "                'democracy since the Civil War. But they failed.',\n",
      "                'America stood strong--and democracy prevailed.',\n",
      "                'But we must be honest--the threat remains--and democracy must '\n",
      "                'be defended.',\n",
      "                'My predecessor and some of you here--seek to bury the truth '\n",
      "                'of January 6th.',\n",
      "                'I will not do that.',\n",
      "                'This is a moment--to speak the truth and bury the lies.',\n",
      "                \"And here\\\\\\\\\\\\'s the simplest truth.\",\n",
      "                \"You can\\\\\\\\\\\\'t love your country--only when you win.\",\n",
      "                \"As I\\\\\\\\\\\\'ve done ever since being elected to office--I ask \"\n",
      "                'you all-- without regard to party--to join together--and '\n",
      "                'defend our democracy!',\n",
      "                'Remember--your oath of office--to defend against all '\n",
      "                'threats--foreign and domestic.',\n",
      "                'Respect free and fair elections!',\n",
      "                'Restore trust in our institutions!',\n",
      "                'And make clear--political violence--has no place in America!',\n",
      "                'History is watching.',\n",
      "                'And history is watching another assault on freedom.',\n",
      "                'Joining us tonight--is Latorya Beasley--a social worker from '\n",
      "                'Birmingham, Alabama.',\n",
      "                '14 months ago tonight--she and her husband welcomed a baby '\n",
      "                'girl-- thanks to the miracle of IVF.',\n",
      "                'She scheduled treatments to have a second child.',\n",
      "                'But the Alabama Supreme Court shut down IVF treatments across '\n",
      "                'the State unleashed by the Supreme Court decision overturning '\n",
      "                'Roe v. Wade.',\n",
      "                'She was told her dream would have to wait.',\n",
      "                'What her family has gone through should never have happened.',\n",
      "                'And unless the Congress acts--it could happen again.',\n",
      "                \"So tonight--let\\\\\\\\\\\\'s stand up for families like hers!\",\n",
      "                \"To my friends across the aisle--don\\\\\\\\\\\\'t keep families \"\n",
      "                'waiting any longer.',\n",
      "                'Guarantee the right to IVF nationwide!',\n",
      "                'Like most Americans--I believe--Roe v. Wade got it right.',\n",
      "                'And I thank Vice President Harris for being an incredible '\n",
      "                'leader defending reproductive freedom and so much more.',\n",
      "                'But my predecessor--came to office determined to see Roe v. '\n",
      "                'Wade overturned.',\n",
      "                \"He\\\\\\\\\\\\'s the reason it was.\",\n",
      "                'In fact--he brags about it.',\n",
      "                'Look at the chaos.',\n",
      "                'Joining us tonight is Kate Cox--a wife and mother from '\n",
      "                'Dallas.',\n",
      "                'When she became pregnant again the fetus had a fatal '\n",
      "                'condition.',\n",
      "                'Her doctors told Kate that her own life--and her ability to '\n",
      "                'have children in the future--were at risk.',\n",
      "                'Because Texas law banned abortion Kate and her husband had to '\n",
      "                'leave the State to get the care she needed.',\n",
      "                'What her family has gone through should never have happened '\n",
      "                'as well.',\n",
      "                'But it is happening to so many others.',\n",
      "                'There are State laws banning abortion--criminalizing '\n",
      "                'doctors--and forcing survivors of rape and incest to leave '\n",
      "                'their States as well.',\n",
      "                'Many of you in this chamber--and my predecessor--are '\n",
      "                'promising to pass a national ban on reproductive freedom.',\n",
      "                'My God--what freedoms will you take next?',\n",
      "                'In its decision to overturn Roe v. Wade--the Supreme Court '\n",
      "                'majority wrote--``Women are not without electoral or '\n",
      "                \"political power.\\\\\\\\\\\\'\\\\\\\\\\\\' No kidding.\",\n",
      "                'Clearly those bragging about overturning Roe v. Wade have no '\n",
      "                'clue about the power of women in America.',\n",
      "                'They found out though--when reproductive freedom was on the '\n",
      "                'ballot and won--in 2022--2023--and they will find out '\n",
      "                'again--in 2024.',\n",
      "                'If Americans send me a Congress that supports the right to '\n",
      "                'choose--I promise you--I will restore Roe v. Wade--as the law '\n",
      "                'of the land-- again!',\n",
      "                'America--cannot go back.',\n",
      "                'I am here tonight--to show the way forward.',\n",
      "                \"Because I know how far we\\\\\\\\\\\\'ve come.\",\n",
      "                'Four years ago next week--before I came to office--our '\n",
      "                'country was hit by the worst pandemic and the worst economic '\n",
      "                'crisis in a century.',\n",
      "                'Remember the fear.',\n",
      "                'Record job losses.',\n",
      "                'A record spike in murder.',\n",
      "                'A raging virus that would take more than 1 million American '\n",
      "                'lives and leave millions of loved ones behind.',\n",
      "                'A mental health crisis of isolation and loneliness.',\n",
      "                'A President--my predecessor--who failed the most basic duty '\n",
      "                'any President owes the American people--the duty to care.',\n",
      "                'That is unforgivable.',\n",
      "                'I came to office determined to get us through one of the '\n",
      "                \"toughest periods in our Nation\\\\\\\\\\\\'s history.\",\n",
      "                'And we have.',\n",
      "                \"It doesn\\\\\\\\\\\\'t make the news--but in thousands of cities \"\n",
      "                'and towns--the American people are writing the greatest '\n",
      "                'comeback story never told.',\n",
      "                \"So let\\\\\\\\\\\\'s tell that story--here and now.\"],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 103.44, 504.0, 395.5199999999999],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 16,\n",
      "  'level': 1,\n",
      "  'page_idx': 4,\n",
      "  'sentences': [\"America\\\\\\\\\\\\'s comeback is building a future of American \"\n",
      "                'possibilities-- building an economy from the middle out and '\n",
      "                'the bottom up--not the top down--investing in all of '\n",
      "                'America--in all Americans--to make sure everyone has a fair '\n",
      "                'shot and we leave no one behind!',\n",
      "                'The pandemic no longer controls our lives.',\n",
      "                'The vaccines that saved us from COVID are now being used to '\n",
      "                'help beat cancer.',\n",
      "                'Turning setback into comeback.',\n",
      "                \"That\\\\\\\\\\\\'s America!\",\n",
      "                'I inherited an economy that was on the brink--now our economy '\n",
      "                'is the envy of the world!',\n",
      "                \"15 million new jobs in just 3 years--that\\\\\\\\\\\\'s a record.\",\n",
      "                'Unemployment at 50-year lows.',\n",
      "                'A record 16 million Americans are starting small businesses '\n",
      "                'and each one is an act of hope.',\n",
      "                'With historic job growth and small business growth for '\n",
      "                'Black--Hispanic--and Asian- Americans.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 399.12, 504.0, 519.8399999999999],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 17,\n",
      "  'level': 1,\n",
      "  'page_idx': 4,\n",
      "  'sentences': ['800,000 new manufacturing jobs in America and counting.',\n",
      "                'More people have health insurance than ever before.',\n",
      "                \"The racial wealth gap is the smallest it\\\\\\\\\\\\'s been in 20 \"\n",
      "                'years.',\n",
      "                'Wages keep going up and inflation keeps coming down!',\n",
      "                'Inflation has dropped from 9 percent to 3 percent--the lowest '\n",
      "                'in the world!',\n",
      "                'And now--instead of importing foreign products and exporting '\n",
      "                'American'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 539.04, 174.0, 551.04],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 18,\n",
      "  'level': 1,\n",
      "  'page_idx': 4,\n",
      "  'sentences': ['[[Page S2274]]'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 570.24, 504.0, 613.44],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 19,\n",
      "  'level': 1,\n",
      "  'page_idx': 4,\n",
      "  'sentences': [\"jobs--we\\\\\\\\\\\\'re exporting American products and creating \"\n",
      "                'American jobs-- right here in America.',\n",
      "                'Where they belong!',\n",
      "                'And the American people are beginning to feel it.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 616.8, 486.0, 660.0],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 20,\n",
      "  'level': 1,\n",
      "  'page_idx': 4,\n",
      "  'sentences': ['Buy American has been the law of the land since the 1930s.',\n",
      "                'Past administrations--including my predecessor--failed to Buy '\n",
      "                'American.',\n",
      "                'Not.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 663.6, 504.0, 753.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 21,\n",
      "  'level': 1,\n",
      "  'page_idx': 4,\n",
      "  'sentences': ['Any.',\n",
      "                'More.',\n",
      "                'On my watch--Federal projects like helping to build American '\n",
      "                'roads--bridges--and highways will be made with American '\n",
      "                'products-- built by American workers--creating good-paying '\n",
      "                'American jobs!',\n",
      "                'Thanks to my CHIPS and Science Act--the United States is '\n",
      "                'investing more in research and development than ever before.',\n",
      "                'During the pandemic--a shortage of semiconductor chips drove '\n",
      "                'up prices for everything--from cell phones to automobiles.',\n",
      "                'Well--instead of having to import semiconductor chips--which '\n",
      "                'America invented I might add-- private companies are now '\n",
      "                'investing billions of dollars to build new chip factories '\n",
      "                'here in America!',\n",
      "                'Creating tens of thousands of jobs--many of them paying over '\n",
      "                \"$100,000 a year and don\\\\\\\\\\\\'t require a college degree.\",\n",
      "                'In fact--my policies have attracted $650 billion of private '\n",
      "                'sector investments in clean energy and advanced '\n",
      "                'manufacturing--creating tens of thousands of jobs here in '\n",
      "                'America!',\n",
      "                'Thanks to our Bipartisan Infrastructure Law--46,000 new '\n",
      "                'projects have been announced across your communities '\n",
      "                'modernizing our roads and bridges--ports and airports--and '\n",
      "                'public transit systems.',\n",
      "                'Removing poisonous lead pipes--so every child can drink clean '\n",
      "                'water without risk of getting brain damage.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 383.52, 504.0, 426.48],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 22,\n",
      "  'level': 1,\n",
      "  'page_idx': 5,\n",
      "  'sentences': ['Providing affordable--high speed Internet for every '\n",
      "                'American--no matter where you live.',\n",
      "                'Urban--suburban--and rural communities--in red States and '\n",
      "                'blue.',\n",
      "                'Record investments in tribal communities.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 430.08, 498.0, 488.88],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 23,\n",
      "  'level': 1,\n",
      "  'page_idx': 5,\n",
      "  'sentences': ['Because of my investments--family farms are better able to '\n",
      "                'stay in the family--and children and grandchildren '\n",
      "                \"won\\\\\\\\\\\\'t have to leave home to make a living.\",\n",
      "                \"It\\\\\\\\\\\\'s transformative.\"],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 492.48, 504.0, 613.44],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 24,\n",
      "  'level': 1,\n",
      "  'page_idx': 5,\n",
      "  'sentences': ['A great comeback story--is Belvidere, Illinois.',\n",
      "                'Home to an auto plant for nearly 60 years.',\n",
      "                'Before I came to office--the plant was on its way to shutting '\n",
      "                'down.',\n",
      "                'Thousands of workers feared for their livelihoods.',\n",
      "                'Hope was fading.',\n",
      "                'Then I was elected to office and we raised Belvidere '\n",
      "                'repeatedly--with the auto company--knowing unions make all '\n",
      "                'the difference.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 616.8, 504.0, 753.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 25,\n",
      "  'level': 1,\n",
      "  'page_idx': 5,\n",
      "  'sentences': ['The UAW worked like hell to keep the plant open and get those '\n",
      "                'jobs back.',\n",
      "                'And together--we succeeded!',\n",
      "                'Instead of an auto factory shutting down an auto factory is '\n",
      "                're-opening--and a new state-of-the art battery factory is '\n",
      "                'being built to power those cars.',\n",
      "                \"Instead of a town being left behind--it\\\\\\\\\\\\'s a community \"\n",
      "                'moving forward again!',\n",
      "                'Because instead of watching auto jobs of the future go '\n",
      "                'overseas--4,000 union workers-- with higher wages--will be '\n",
      "                'building that future--here in America!',\n",
      "                'Here tonight--is UAW President Shawn Fain--a great friend and '\n",
      "                'a great labor leader.',\n",
      "                'And Dawn Simms--a third generation UAW worker in Belvidere.',\n",
      "                'Shawn--I was proud to be the first President in American '\n",
      "                'history to walk a picket line.',\n",
      "                'And today--Dawn has a job in her hometown providing stability '\n",
      "                'for her family and pride and dignity.',\n",
      "                \"Showing once again Wall Street didn\\\\\\\\\\\\'t build this \"\n",
      "                'country!',\n",
      "                'The middle class built this country!',\n",
      "                'And unions built the middle class!',\n",
      "                'When Americans get knocked down--we get back up!',\n",
      "                'We keep going!',\n",
      "                \"That\\\\\\\\\\\\'s America!\",\n",
      "                \"That\\\\\\\\\\\\'s you--the American people!\",\n",
      "                \"It\\\\\\\\\\\\'s because of you America is coming back!\",\n",
      "                \"It\\\\\\\\\\\\'s because of you our future is brighter!\",\n",
      "                \"And it\\\\\\\\\\\\'s because of you--That tonight--we can proudly \"\n",
      "                'say--the State of our Union is strong and getting stronger!',\n",
      "                'Tonight--I want to talk about the future of possibilities '\n",
      "                'that we can build together.',\n",
      "                'A future where the days of trickledown economics are over and '\n",
      "                'the wealthy and biggest corporations no longer get all the '\n",
      "                'breaks.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 399.12, 498.0, 488.88],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 26,\n",
      "  'level': 1,\n",
      "  'page_idx': 6,\n",
      "  'sentences': ['I grew up in a home where not a lot trickled down on my '\n",
      "                \"dad\\\\\\\\\\\\'s kitchen table.\",\n",
      "                \"That\\\\\\\\\\\\'s why I\\\\\\\\\\\\'m determined to turn things around \"\n",
      "                'so the middle class does well--the poor have a way up and the '\n",
      "                'wealthy still does well.',\n",
      "                'We all do well.',\n",
      "                \"And there\\\\\\\\\\\\'s more to do to make sure you\\\\\\\\\\\\'re \"\n",
      "                \"feeling the benefits of all we\\\\\\\\\\\\'re doing.\"],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 492.48, 504.0, 722.1600000000001],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 27,\n",
      "  'level': 1,\n",
      "  'page_idx': 6,\n",
      "  'sentences': ['Americans pay more for prescription drugs than anywhere else.',\n",
      "                \"It\\\\\\\\\\\\'s wrong and I\\\\\\\\\\\\'m ending it.\",\n",
      "                'With a law I proposed and signed and not one Republican voted '\n",
      "                'for--we finally beat Big Pharma!',\n",
      "                'Instead of paying $400 a month for insulin--seniors with '\n",
      "                'diabetes only have to pay $35 a month!',\n",
      "                'And now--I want to cap the cost of insulin at $35 a month for '\n",
      "                'every American who needs it!',\n",
      "                'For years--people have talked about it--but I finally got it '\n",
      "                'done-- and gave Medicare the power to negotiate lower prices '\n",
      "                'for prescription drugs--just like the VA does for our '\n",
      "                'veterans.',\n",
      "                \"That\\\\\\\\\\\\'s not just saving seniors money.\",\n",
      "                \"It\\\\\\\\\\\\'s saving taxpayers money--cutting the Federal \"\n",
      "                'deficit by $160 billion--because Medicare will no longer have '\n",
      "                'to pay exorbitant prices to Big Pharma.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 725.76, 504.0, 737.76],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 28,\n",
      "  'level': 1,\n",
      "  'page_idx': 6,\n",
      "  'sentences': ['This year--Medicare is negotiating lower prices for some of '\n",
      "                'the costliest drugs on the market that treat everything from '\n",
      "                'heart disease to arthritis.',\n",
      "                \"Now it\\\\\\\\\\\\'s time to go further and give Medicare the power \"\n",
      "                'to negotiate lower prices for 500 drugs over the next decade.',\n",
      "                'That will not only save lives--it will save taxpayers another '\n",
      "                '$200 billion!',\n",
      "                'Starting next year--that same law caps total prescription '\n",
      "                'drug costs for seniors on Medicare at $2,000 a year--even for '\n",
      "                'expensive cancer drugs that can cost '\n",
      "                '$10,000--$12,000--$15,000 a year.',\n",
      "                'Now--I want to cap prescription drug costs at $2,000 a '\n",
      "                'year--for everyone!',\n",
      "                'Folks--the Affordable Care Act is still a very big deal.',\n",
      "                'Over one hundred million of you can no longer be denied '\n",
      "                'health insurance because of pre-existing conditions.',\n",
      "                'But my predecessor--and many in this chamber--want to take '\n",
      "                'that protection away by repealing the Affordable Care Act. I '\n",
      "                \"won\\\\\\\\\\\\'t let that happen!\",\n",
      "                'We stopped you 50 times before-- and we will stop you again!',\n",
      "                'In fact--I am protecting it and expanding it.',\n",
      "                'I enacted tax credits-- that save $800 per person--per '\n",
      "                'year--reducing health care premiums for millions of working '\n",
      "                'families.',\n",
      "                'Those tax credits expire next year.',\n",
      "                'I want to make those savings permanent!',\n",
      "                'Women are more than half of our population but research on '\n",
      "                \"women\\\\\\\\\\\\'s health has always been underfunded.\",\n",
      "                \"That\\\\\\\\\\\\'s why we\\\\\\\\\\\\'re launching the first-ever White \"\n",
      "                \"House Initiative on Women\\\\\\\\\\\\'s Health Research--led by \"\n",
      "                'Jill--who is doing an incredible job as First Lady.',\n",
      "                \"Pass my plan for $12 billion--to transform women\\\\\\\\\\\\'s \"\n",
      "                'health research and benefit millions of lives across America!',\n",
      "                'I know the cost of housing is so important to you.',\n",
      "                'If inflation keeps coming down--mortgage rates--will come '\n",
      "                'down as well.',\n",
      "                \"But I\\\\\\\\\\\\'m not waiting.\",\n",
      "                'I want to provide an annual tax credit that will give '\n",
      "                'Americans $400 a month--for the next 2 years--as mortgage '\n",
      "                'rates come down--to put toward their mortgage when they buy a '\n",
      "                'first home or trade up for a little more space.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 710.16, 504.0, 753.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 29,\n",
      "  'level': 1,\n",
      "  'page_idx': 7,\n",
      "  'sentences': ['My Administration is also eliminating title insurance fees '\n",
      "                'for federally backed mortgages.',\n",
      "                'When you refinance your home--this can save you $1,000 or '\n",
      "                'more.',\n",
      "                \"For millions of renters--we\\\\\\\\\\\\'re cracking down on big \"\n",
      "                'landlords who break antitrust laws by price-fixing and '\n",
      "                'driving up rents.',\n",
      "                \"I\\\\\\\\\\\\'ve cut red tape--so more builders can get Federal \"\n",
      "                'financing- - which is already helping build a record 1.7 '\n",
      "                'million housing units nationwide.',\n",
      "                'Now pass my plan to build and renovate 2 million affordable '\n",
      "                'homes--and bring those rents down!',\n",
      "                'To remain the strongest economy in the world--we need the '\n",
      "                'best education system in the world.',\n",
      "                'I want to give every child a good start by providing access '\n",
      "                'to pre-school for 3- and 4-year-olds.',\n",
      "                'Studies show that children who go to pre-school are nearly 50 '\n",
      "                'percent more likely to finish high school and go on to earn a '\n",
      "                '2- or 4-year degree no matter their background.',\n",
      "                'I want to expand high-quality tutoring and summer learning '\n",
      "                'time and see to it that every child learns to read by third '\n",
      "                'grade.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 383.52, 492.0, 519.8399999999999],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 30,\n",
      "  'level': 1,\n",
      "  'page_idx': 8,\n",
      "  'sentences': [\"I\\\\\\\\\\\\'m also connecting local businesses and high schools \"\n",
      "                'so students get hands-on experience and a path to a '\n",
      "                'good-paying job whether or not they go to college.',\n",
      "                'And I want to make college more affordable.',\n",
      "                \"Let\\\\\\\\\\\\'s continue increasing Pell Grants for working- and \"\n",
      "                'middle-class families and increase our record investments in '\n",
      "                'HBCUs and Minority-Serving Institutions.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 523.44, 504.0, 551.04],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 31,\n",
      "  'level': 1,\n",
      "  'page_idx': 8,\n",
      "  'sentences': ['I fixed student loan programs--to reduce the burden of '\n",
      "                'student debt for nearly 4 million Americans--including'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 570.24, 174.0, 582.24],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 32,\n",
      "  'level': 1,\n",
      "  'page_idx': 8,\n",
      "  'sentences': ['[[Page S2275]]'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 601.44, 504.0, 753.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 33,\n",
      "  'level': 1,\n",
      "  'page_idx': 8,\n",
      "  'sentences': ['nurses--firefighters--and others in public service--like '\n",
      "                'Keenan Jones-- a public-school educator in '\n",
      "                \"Minnesota--who\\\\\\\\\\\\'s here with us tonight.\",\n",
      "                \"He\\\\\\\\\\\\'s educated hundreds of students so they can go to \"\n",
      "                'college--now he can help his own daughter pay for college.',\n",
      "                'Such relief is good for the economy because folks are now '\n",
      "                'able to buy a home--start a busines-- even start a family.',\n",
      "                \"While we\\\\\\\\\\\\'re at it--I want to give public school \"\n",
      "                'teachers a raise!',\n",
      "                'Now--let me speak to a question of fundamental fairness for '\n",
      "                'all Americans.',\n",
      "                \"I\\\\\\\\\\\\'ve been delivering real results in a fiscally \"\n",
      "                'responsible way.',\n",
      "                \"I\\\\\\\\\\\\'ve already cut the Federal deficit by over one \"\n",
      "                'trillion dollars.',\n",
      "                'I signed a bipartisan budget deal that will cut another '\n",
      "                'trillion dollars over the next decade.',\n",
      "                \"And now--it\\\\\\\\\\\\'s my goal to cut the Federal deficit $3 \"\n",
      "                'trillion more by making big corporations and the very wealthy '\n",
      "                'finally pay their fair share.',\n",
      "                \"Look--I\\\\\\\\\\\\'m a capitalist.\",\n",
      "                'If you want to make a million bucks-- great!',\n",
      "                'Just pay your fair share in taxes.',\n",
      "                'A fair tax code is how we invest in the things that make a '\n",
      "                'country great--health care-- education--defense--and more.',\n",
      "                \"But here\\\\\\\\\\\\'s the deal.\",\n",
      "                'The last administration enacted a $2 trillion tax cut that '\n",
      "                'overwhelmingly benefits the very wealthy and the biggest '\n",
      "                'corporations--and exploded the Federal deficit.',\n",
      "                'They added more to the national debt than in any Presidential '\n",
      "                'term in American history.',\n",
      "                'For folks at home--does anybody really think the tax code is '\n",
      "                'fair?',\n",
      "                'Do you really think the wealthy and big corporations need '\n",
      "                'another $2 trillion in tax breaks?',\n",
      "                \"I sure don\\\\\\\\\\\\'t. I\\\\\\\\\\\\'m going to keep fighting like \"\n",
      "                'hell to make it fair!',\n",
      "                'Under my plan--nobody earning less than $400,000 will pay an '\n",
      "                'additional penny in Federal taxes.',\n",
      "                'Nobody.',\n",
      "                'Not one penny.',\n",
      "                'In fact-- the Child Tax Credit I passed during the '\n",
      "                'pandemic--cut taxes for millions of working families and cut '\n",
      "                'child poverty in half.',\n",
      "                'Restore the Child Tax Credit--because no child should go '\n",
      "                'hungry in this country!',\n",
      "                'The way to make the tax code fair is to make big corporations '\n",
      "                'and the very wealthy finally pay their share.',\n",
      "                'In 2020--55 of the biggest companies in America made $40 '\n",
      "                'billion in profits and paid zero in Federal income taxes.',\n",
      "                'Not anymore!',\n",
      "                'Thanks to the law I wrote and signed--big companies now have '\n",
      "                'to pay a minimum of 15 percent.',\n",
      "                \"But that\\\\\\\\\\\\'s still less than working people pay in \"\n",
      "                'Federal taxes.',\n",
      "                \"It\\\\\\\\\\\\'s time to raise the corporate minimum tax to at \"\n",
      "                'least 21 percent so every big corporation finally begins to '\n",
      "                'pay their fair share.',\n",
      "                'I also want to end the tax breaks for Big Pharma--Big '\n",
      "                'Oil--private jets--and massive executive pay!',\n",
      "                'End it now! There are 1,000 billionaires in America.',\n",
      "                'You know what the average Federal tax rate for these '\n",
      "                'billionaires is?',\n",
      "                '8.2 percent!',\n",
      "                \"That\\\\\\\\\\\\'s far less than the vast majority of Americans \"\n",
      "                'pay.',\n",
      "                'No billionaire should pay a lower tax rate than a teacher--a '\n",
      "                'sanitation worker--a nurse!',\n",
      "                \"That\\\\\\\\\\\\'s why I\\\\\\\\\\\\'ve proposed a minimum tax of 25 \"\n",
      "                'percent for billionaires.',\n",
      "                'Just 25 percent.',\n",
      "                'That would raise $500 billion over the next 10 years.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 196.8, 498.0, 286.56],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 34,\n",
      "  'level': 1,\n",
      "  'page_idx': 10,\n",
      "  'sentences': ['Imagine what that could do for America.',\n",
      "                'Imagine a future with affordable child care--so millions of '\n",
      "                'families can get the care they need and still go to work and '\n",
      "                'help grow the economy.',\n",
      "                'Imagine a future with paid leave--because no one should have '\n",
      "                'to choose between working and taking care of yourself or a '\n",
      "                'sick family member.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 290.16, 511.3, 753.3600000000001],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 35,\n",
      "  'level': 1,\n",
      "  'page_idx': 10,\n",
      "  'sentences': ['Imagine a future with home care and elder care--so seniors '\n",
      "                'and people living with disabilities can stay in their homes '\n",
      "                'and family caregivers get paid what they deserve!',\n",
      "                \"Tonight--let\\\\\\\\\\\\'s all agree once again to stand up for \"\n",
      "                'seniors!',\n",
      "                'Many of my Republican friends want to put Social Security on '\n",
      "                'the chopping block.',\n",
      "                'If anyone here tries to cut Social Security or Medicare--or '\n",
      "                'raise the retirement age--I will stop them!',\n",
      "                'Working people who built this country pay more into Social '\n",
      "                'Security than millionaires and billionaires do.',\n",
      "                \"It\\\\\\\\\\\\'s not fair.\",\n",
      "                'We have two ways to go on Social Security.',\n",
      "                'Republicans will cut Social Security and give more tax cuts '\n",
      "                'to the wealthy.',\n",
      "                'I will protect and strengthen Social Security--and make the '\n",
      "                'wealthy pay their fair share!',\n",
      "                'Too many corporations raise their prices to pad their '\n",
      "                'profits-- charging you more and more for less and less.',\n",
      "                \"That\\\\\\\\\\\\'s why we\\\\\\\\\\\\'re cracking down on corporations \"\n",
      "                'that engage in price gouging or deceptive pricing from food '\n",
      "                'to health care to housing.',\n",
      "                \"In fact--snack companies think you won\\\\\\\\\\\\'t notice when \"\n",
      "                'they charge you just as much for the same size bag--but with '\n",
      "                'fewer chips in it.',\n",
      "                \"Pass Senator Bob Casey\\\\\\\\\\\\'s bill to put a stop to \"\n",
      "                'shrink-flation!',\n",
      "                \"I\\\\\\\\\\\\'m also getting rid of junk fees--those hidden fees \"\n",
      "                'added at the end of your bills without your knowledge.',\n",
      "                \"My Administration just announced--we\\\\\\\\\\\\'re cutting credit \"\n",
      "                'card late fees from $32 to just $8.',\n",
      "                \"The banks and credit card companies don\\\\\\\\\\\\'t like it.\",\n",
      "                'Why?',\n",
      "                \"I\\\\\\\\\\\\'m saving American families $20 billion a year with \"\n",
      "                \"all of the junk fees I\\\\\\\\\\\\'m eliminating.\",\n",
      "                \"And I\\\\\\\\\\\\'m not stopping there.\",\n",
      "                'My Administration has proposed rules to make '\n",
      "                'cable--travel--utilities--and online ticket sellers tell you '\n",
      "                'the total price upfront--so there are no surprises.',\n",
      "                'It matters.',\n",
      "                'And so does this.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 227.76, 504.0, 270.96],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 36,\n",
      "  'level': 1,\n",
      "  'page_idx': 11,\n",
      "  'sentences': ['In November--my team began serious negotiations with a '\n",
      "                'bipartisan group of Senators.',\n",
      "                'The result was a bipartisan bill with the toughest set of '\n",
      "                \"border security reforms we\\\\\\\\\\\\'ve ever seen in this \"\n",
      "                'country.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 274.56, 504.0, 411.12],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 37,\n",
      "  'level': 1,\n",
      "  'page_idx': 11,\n",
      "  'sentences': ['That bipartisan deal would hire 1,500 more border security '\n",
      "                'agents and officers.',\n",
      "                '100 more immigration judges to help tackle a backload of 2 '\n",
      "                'million cases.',\n",
      "                '4,300 more asylum officers--and new policies so they can '\n",
      "                'resolve cases in 6 months instead of 6 years.',\n",
      "                '100 more high-tech drug detection machines to significantly '\n",
      "                'increase the ability to screen and stop vehicles from '\n",
      "                'smuggling fentanyl into America.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 414.48, 504.0, 488.88],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 38,\n",
      "  'level': 1,\n",
      "  'page_idx': 11,\n",
      "  'sentences': ['This bill would save lives.',\n",
      "                'And bring order to the border.',\n",
      "                'It would also give me--as President--new emergency authority '\n",
      "                'to temporarily shut down the border when the number of '\n",
      "                'migrants at the border is overwhelming.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 492.48, 498.0, 551.04],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 39,\n",
      "  'level': 1,\n",
      "  'page_idx': 11,\n",
      "  'sentences': ['The Border Patrol Union endorsed the bill.',\n",
      "                'The Chamber of Commerce endorsed the bill.',\n",
      "                'I believe that given the opportunity--a majority of the House '\n",
      "                'and Senate would endorse it as well.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 554.64, 486.0, 628.8],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 40,\n",
      "  'level': 1,\n",
      "  'page_idx': 11,\n",
      "  'sentences': ['But unfortunately--politics have derailed it so far.',\n",
      "                \"I\\\\\\\\\\\\'m told my predecessor called Republicans in Congress \"\n",
      "                'and demanded they block the bill.',\n",
      "                'He feels it would be a political win for me--and a political '\n",
      "                'loser for him.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 632.4, 504.0, 753.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 41,\n",
      "  'level': 1,\n",
      "  'page_idx': 11,\n",
      "  'sentences': [\"It\\\\\\\\\\\\'s not about him or me.\",\n",
      "                \"It\\\\\\\\\\\\'d be a winner for America!\",\n",
      "                'My Republican friends--you owe it to the American people to '\n",
      "                'get this bill done.',\n",
      "                'We need to act. And if my predecessor is watching--instead of '\n",
      "                'playing politics and pressuring Members of Congress to block '\n",
      "                'this bill--join me in telling Congress to pass it!',\n",
      "                'We can do it together.',\n",
      "                \"But here\\\\\\\\\\\\'s what I will not do.\",\n",
      "                'I will not demonize immigrants--saying they ``poison the '\n",
      "                \"blood of our country\\\\\\\\\\\\'\\\\\\\\\\\\'- - as he said in his own \"\n",
      "                'words.',\n",
      "                'I will not separate families.',\n",
      "                'I will not ban people from America because of their faith.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 134.4, 504.0, 193.2],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 42,\n",
      "  'level': 1,\n",
      "  'page_idx': 12,\n",
      "  'sentences': ['Unlike my predecessor--on my first day in office I introduced '\n",
      "                'a comprehensive plan to fix our immigration system--secure '\n",
      "                'the border-- and provide a pathway to citizenship for '\n",
      "                'DREAMERS--and so much more.',\n",
      "                'Because--unlike my predecessor--I know who we are as '\n",
      "                'Americans.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 196.8, 511.3, 333.12],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 43,\n",
      "  'level': 1,\n",
      "  'page_idx': 12,\n",
      "  'sentences': ['We are the only Nation in the world with a heart and '\n",
      "                'soul--that draws from old and new. Home to Native Americans '\n",
      "                'whose ancestors have been here for thousands of years.',\n",
      "                'Home to people from every place on Earth.',\n",
      "                'Some came freely.',\n",
      "                'Some chained by force.',\n",
      "                'Some when famine struck-- like my ancestral family in '\n",
      "                'Ireland.',\n",
      "                'Some to flee persecution.',\n",
      "                'Some to chase dreams that are impossible anywhere but here in '\n",
      "                'America.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 336.72, 504.0, 457.68],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 44,\n",
      "  'level': 1,\n",
      "  'page_idx': 12,\n",
      "  'sentences': [\"That\\\\\\\\\\\\'s America--where we all come from somewhere--but \"\n",
      "                'we are all Americans.',\n",
      "                'We can fight about the border--or we can fix it.',\n",
      "                \"I\\\\\\\\\\\\'m ready to fix it.\",\n",
      "                'Send me the border bill now! A transformational moment in our '\n",
      "                'history happened 59 years ago today--in Selma, Alabama.',\n",
      "                'Hundreds of foot soldiers for justice marched across the '\n",
      "                'Edmund Pettus Bridge--named after a Grand Dragon of the '\n",
      "                'KKK--to claim their fundamental right to vote.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 461.28, 504.0, 566.6400000000001],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 45,\n",
      "  'level': 1,\n",
      "  'page_idx': 12,\n",
      "  'sentences': ['They were beaten--bloodied--and left for dead.',\n",
      "                'Our late friend and former colleague John Lewis was at the '\n",
      "                'march.',\n",
      "                'We miss him. Joining us tonight are other marchers who were '\n",
      "                'there--including Bettie Mae Fikes-- known as the--``Voice of '\n",
      "                \"Selma\\\\\\\\\\\\'\\\\\\\\\\\\'.\",\n",
      "                'A daughter of gospel singers and preachers--she sang songs of '\n",
      "                'prayer and protest on that Bloody Sunday to help'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 585.84, 174.0, 597.84],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 46,\n",
      "  'level': 1,\n",
      "  'page_idx': 12,\n",
      "  'sentences': ['[[Page S2276]]'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 616.8, 492.0, 660.0],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 47,\n",
      "  'level': 1,\n",
      "  'page_idx': 12,\n",
      "  'sentences': [\"shake the Nation\\\\\\\\\\\\'s conscience.\",\n",
      "                'Five months later--the Voting Rights Act was signed into '\n",
      "                'law.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 663.6, 498.0, 706.8],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 48,\n",
      "  'level': 1,\n",
      "  'page_idx': 12,\n",
      "  'sentences': ['But 59 years later--there are forces taking us back in time.',\n",
      "                'Voter suppression.',\n",
      "                'Election subversion.',\n",
      "                'Unlimited dark money.',\n",
      "                'Extreme gerrymandering.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 710.16, 504.0, 737.76],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 49,\n",
      "  'level': 1,\n",
      "  'page_idx': 12,\n",
      "  'sentences': ['John Lewis was a great friend to many of us here.',\n",
      "                'But if you truly want to honor him and all the heroes who '\n",
      "                \"marched with him--then it\\\\\\\\\\\\'s time for more than just \"\n",
      "                'talk.',\n",
      "                'Pass and send me the Freedom to Vote Act and the John Lewis '\n",
      "                'voting Rights Act! And stop denying another core value of '\n",
      "                'America--our diversity-- across American life.',\n",
      "                'Banning books.',\n",
      "                \"It\\\\\\\\\\\\'s wrong!\",\n",
      "                \"Instead of erasing history- - let\\\\\\\\\\\\'s make history!\",\n",
      "                'I want to protect other fundamental rights!',\n",
      "                'Pass the Equality Act-- and my message to transgender '\n",
      "                'Americans--I have your back!',\n",
      "                \"Pass the PRO Act for workers\\\\\\\\\\\\' rights!\",\n",
      "                'And raise the Federal minimum wage--because every worker has '\n",
      "                'the right to earn a decent living!',\n",
      "                'We are also making history by confronting the climate '\n",
      "                'crisis--not denying it.',\n",
      "                \"I\\\\\\\\\\\\'m taking the most significant action on climate ever \"\n",
      "                'in the history of the world.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 321.12, 504.0, 411.12],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 50,\n",
      "  'level': 1,\n",
      "  'page_idx': 13,\n",
      "  'sentences': ['I am cutting our carbon emissions in half by 2030.',\n",
      "                'Creating tens of thousands of clean-energy jobs--like the '\n",
      "                'IBEW workers building and installing 500,000 electric vehicle '\n",
      "                'charging stations.',\n",
      "                \"Conserving 30 percent of America\\\\\\\\\\\\'s lands and waters by \"\n",
      "                '2030.',\n",
      "                'Taking historic action on environmental justice for '\n",
      "                'fence-line communities smothered by the legacy of pollution.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 414.48, 504.0, 473.28],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 51,\n",
      "  'level': 1,\n",
      "  'page_idx': 13,\n",
      "  'sentences': ['And--patterned after the Peace Corps and Ameri '\n",
      "                \"Corps--I\\\\\\\\\\\\'ve launched a Climate Corps to put 20,000 \"\n",
      "                'young people to work at the forefront of our clean energy '\n",
      "                'future.',\n",
      "                \"I\\\\\\\\\\\\'ll triple that number this decade.\"],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 476.88, 504.0, 519.8399999999999],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 52,\n",
      "  'level': 1,\n",
      "  'page_idx': 13,\n",
      "  'sentences': ['All Americans deserve the freedom to be safe.',\n",
      "                'And America is safer today than when I took office.',\n",
      "                'The year before I took office--murders went up 30 percent '\n",
      "                'nationwide--the biggest increase in history.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 523.44, 498.0, 566.64],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 53,\n",
      "  'level': 1,\n",
      "  'page_idx': 13,\n",
      "  'sentences': ['That was then.',\n",
      "                'Now--through my American Rescue Plan--which every Republican '\n",
      "                \"voted against--I\\\\\\\\\\\\'ve made the largest investment in \"\n",
      "                'public safety ever.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 570.24, 504.0, 753.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 54,\n",
      "  'level': 1,\n",
      "  'page_idx': 13,\n",
      "  'sentences': ['Last year--the murder rate saw the sharpest decrease in '\n",
      "                'history.',\n",
      "                'And violent crime fell to one of the lowest levels in more '\n",
      "                'than 50 years.',\n",
      "                'But we have more to do.',\n",
      "                'Help cities and towns invest in more community police '\n",
      "                'officers--more mental health workers--and more community '\n",
      "                'violence intervention.',\n",
      "                'Give communities the tools to crack down on gun crime--retail '\n",
      "                'crime--and carjacking.',\n",
      "                \"Keep building public trust--as I\\\\\\\\\\\\'ve been doing by \"\n",
      "                'taking executive action on police reform and calling for it '\n",
      "                'to be the law of the land!',\n",
      "                'Directing my Cabinet to review the Federal classification of '\n",
      "                'marijuana and expunging thousands of convictions for mere '\n",
      "                'possession--because no one should be jailed just for using or '\n",
      "                'possessing marijuana!',\n",
      "                'To take on domestic violence--I am ramping up Federal '\n",
      "                'enforcement of the Violence Against Women Act that I proudly '\n",
      "                'wrote--so we can finally end the scourge of violence against '\n",
      "                'women in America!',\n",
      "                \"And there\\\\\\\\\\\\'s another kind of violence I want to stop.\",\n",
      "                'With us tonight is Jazmin--whose 9-year-old sister Jackie was '\n",
      "                'murdered with 21 classmates and teachers at her elementary '\n",
      "                'school in Uvalde, Texas.',\n",
      "                'Soon after it happened, Jill and I went to Uvalde and spent '\n",
      "                'hours with the families.',\n",
      "                'We heard their message--and so should everyone in this '\n",
      "                'chamber--do something.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 274.56, 504.0, 551.04],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 55,\n",
      "  'level': 1,\n",
      "  'page_idx': 14,\n",
      "  'sentences': ['I did do something by establishing the first-ever Office of '\n",
      "                'Gun Violence Prevention in the White House that Vice '\n",
      "                'President Harris is leading.',\n",
      "                \"Meanwhile--my predecessor told the NRA he\\\\\\\\\\\\'s proud he \"\n",
      "                'did nothing on guns when he was President.',\n",
      "                'After another school shooting in Iowa--he said we should just '\n",
      "                \"``get over it.\\\\\\\\\\\\'\\\\\\\\\\\\'I say we must stop it.\",\n",
      "                \"I\\\\\\\\\\\\'m proud we beat the NRA when I signed the most \"\n",
      "                'significant gun safety law in nearly 30 years!',\n",
      "                'Now we must beat the NRA again!',\n",
      "                \"I\\\\\\\\\\\\'m demanding a ban on assault weapons and high- \"\n",
      "                'capacity magazines!',\n",
      "                'Pass universal background checks!',\n",
      "                'None of this violates the Second Amendment--or vilifies '\n",
      "                'responsible gun owners.',\n",
      "                \"As we manage challenges at home--we\\\\\\\\\\\\'re also managing \"\n",
      "                'crises abroad- - including in the Middle East.',\n",
      "                'I know the last 5 months have been gut- wrenching for so many '\n",
      "                'people--for the Israeli people--the Palestinian people--and '\n",
      "                'so many here in America.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 554.64, 504.0, 706.8],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 56,\n",
      "  'level': 1,\n",
      "  'page_idx': 14,\n",
      "  'sentences': ['This crisis began on October 7th with a massacre by the '\n",
      "                'terrorist group Hamas.',\n",
      "                '1,200 innocent people--women and girls--men and boys-- '\n",
      "                'slaughtered--many enduring sexual violence.',\n",
      "                'The deadliest day for the Jewish people since the Holocaust.',\n",
      "                '250 hostages.',\n",
      "                'Here in the chamber tonight are American families whose loved '\n",
      "                'ones are still being held by Hamas.',\n",
      "                'I pledge to all the families that we will not rest until we '\n",
      "                'bring their loved ones home.',\n",
      "                'We will also work around the clock to bring home Evan and '\n",
      "                'Paul--Americans being unjustly detained all around the '\n",
      "                'world.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 710.16, 504.0, 753.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 57,\n",
      "  'level': 1,\n",
      "  'page_idx': 14,\n",
      "  'sentences': ['Israel has a right to go after Hamas.',\n",
      "                'Hamas could end this conflict today by releasing the '\n",
      "                'hostages--laying down arms--and surrendering those '\n",
      "                'responsible for October 7th.',\n",
      "                'Israel has an added burden because Hamas hides and operates '\n",
      "                'among the civilian population.',\n",
      "                'But Israel also has a fundamental responsibility to protect '\n",
      "                'innocent civilians in Gaza.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 134.4, 511.3, 255.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 58,\n",
      "  'level': 1,\n",
      "  'page_idx': 15,\n",
      "  'sentences': ['This war has taken a greater toll on innocent civilians than '\n",
      "                'all previous wars in Gaza combined.',\n",
      "                'More than 30,000 Palestinians have been killed.',\n",
      "                'Most of whom are not Hamas.',\n",
      "                'Thousands and thousands are innocent women and children.',\n",
      "                'Children also orphaned.',\n",
      "                'Nearly 2 million more Palestinians under bombardment or '\n",
      "                'displaced.',\n",
      "                'Homes destroyed-- neighborhoods in rubble--cities in ruin.',\n",
      "                'Families without food, water, medicine.',\n",
      "                \"It\\\\\\\\\\\\'s heartbreaking.\"],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 258.96, 504.0, 333.12],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 59,\n",
      "  'level': 1,\n",
      "  'page_idx': 15,\n",
      "  'sentences': [\"We\\\\\\\\\\\\'ve been working non-stop to establish an immediate \"\n",
      "                'ceasefire that would last for at least 6 weeks.',\n",
      "                'It would get the hostages home--ease the intolerable '\n",
      "                'humanitarian crisis--and build toward something more '\n",
      "                'enduring.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 336.72, 504.0, 488.88],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 60,\n",
      "  'level': 1,\n",
      "  'page_idx': 15,\n",
      "  'sentences': ['The United States has been leading international efforts to '\n",
      "                'get more humanitarian assistance into Gaza.',\n",
      "                \"Tonight--I\\\\\\\\\\\\'m directing the United States military to \"\n",
      "                'lead an emergency mission to establish a temporary pier in '\n",
      "                'the Mediterranean--on the Gaza coast--that can receive large '\n",
      "                'ships carrying food--water--medicine--and temporary shelters.',\n",
      "                'No U.S. boots will be on the ground.',\n",
      "                'This temporary pier would enable a massive increase in the '\n",
      "                'amount of humanitarian assistance getting into Gaza every '\n",
      "                'day.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 492.48, 504.0, 582.24],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 61,\n",
      "  'level': 1,\n",
      "  'page_idx': 15,\n",
      "  'sentences': ['But Israel must also do its part.',\n",
      "                'Israel must allow more aid into Gaza--and ensure that '\n",
      "                \"humanitarian workers aren\\\\\\\\\\\\'t caught in the cross fire.\",\n",
      "                'To the leadership of Israel, I say this--humanitarian '\n",
      "                'assistance cannot be a secondary consideration or a '\n",
      "                'bargaining chip.',\n",
      "                'Protecting and saving innocent lives has to be a priority.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 585.84, 498.0, 628.8],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 62,\n",
      "  'level': 1,\n",
      "  'page_idx': 15,\n",
      "  'sentences': ['As we look to the future--the only real solution is a '\n",
      "                'two-state solution.',\n",
      "                'I say this as a lifelong supporter of Israel--and the only '\n",
      "                'American President to visit Israel in wartime.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 632.4, 504.0, 753.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 63,\n",
      "  'level': 1,\n",
      "  'page_idx': 15,\n",
      "  'sentences': [\"There is no other path that guarantees Israel\\\\\\\\\\\\'s \"\n",
      "                'security--and democracy.',\n",
      "                'There is no other path that guarantees Palestinians can live '\n",
      "                'with peace and dignity.',\n",
      "                'There is no other path that guarantees peace between Israel '\n",
      "                'and all of its Arab neighbors--including Saudi Arabia.',\n",
      "                'Creating stability in the Middle East also means containing '\n",
      "                'the threat posed by Iran.',\n",
      "                \"That\\\\\\\\\\\\'s why I built a coalition of more than a dozen \"\n",
      "                'countries to defend international shipping and freedom of '\n",
      "                \"navigation in the Red Sea. I\\\\\\\\\\\\'ve ordered strikes to \"\n",
      "                'degrade Houthi capabilities and defend United States Forces '\n",
      "                'in the region.',\n",
      "                'As Commander in Chief--I will not hesitate, to direct further '\n",
      "                'measures to protect our people and military personnel.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 150.0, 498.0, 239.76],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 64,\n",
      "  'level': 1,\n",
      "  'page_idx': 16,\n",
      "  'sentences': [\"For years--all I\\\\\\\\\\\\'ve heard from my Republican \"\n",
      "                \"friends--and so many others--is that China\\\\\\\\\\\\'s on the \"\n",
      "                'rise--and America is falling behind.',\n",
      "                \"They\\\\\\\\\\\\'ve got it backward.\",\n",
      "                'America is rising.',\n",
      "                'We have the best economy in the world.',\n",
      "                \"Since I\\\\\\\\\\\\'ve come to office--our GDP is up.\",\n",
      "                'And our trade deficit with China is down to the lowest point '\n",
      "                'in over a decade.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 243.36, 498.0, 317.76],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 65,\n",
      "  'level': 1,\n",
      "  'page_idx': 16,\n",
      "  'sentences': [\"We\\\\\\\\\\\\'re standing against China\\\\\\\\\\\\'s unfair economic \"\n",
      "                'practices.',\n",
      "                'And standing up for peace and stability across the Taiwan '\n",
      "                'Strait.',\n",
      "                \"I\\\\\\\\\\\\'ve revitalized our partnerships and alliances in the \"\n",
      "                'Pacific.',\n",
      "                \"I\\\\\\\\\\\\'ve made sure that the most advanced American \"\n",
      "                \"technologies can\\\\\\\\\\\\'t be used in China\\\\\\\\\\\\'s weapons.\"],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 321.12, 492.0, 348.72],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 66,\n",
      "  'level': 1,\n",
      "  'page_idx': 16,\n",
      "  'sentences': ['Frankly--for all his tough talk on China--it never occurred '\n",
      "                'to my predecessor to do that.',\n",
      "                'We want competition'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 367.92, 174.0, 379.92],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 67,\n",
      "  'level': 1,\n",
      "  'page_idx': 16,\n",
      "  'sentences': ['[[Page S2277]]'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 399.12, 486.0, 473.28],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 68,\n",
      "  'level': 1,\n",
      "  'page_idx': 16,\n",
      "  'sentences': ['with China--but not conflict.',\n",
      "                'And we are in a stronger position to win the competition for '\n",
      "                'the 2lst Century against China--or anyone else for that '\n",
      "                'matter.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 476.88, 511.3, 675.6000000000001],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 69,\n",
      "  'level': 1,\n",
      "  'page_idx': 16,\n",
      "  'sentences': [\"Here at home--I\\\\\\\\\\\\'ve signed over 400 bipartisan bills.\",\n",
      "                \"But there\\\\\\\\\\\\'s more to do to pass my Unity Agenda.\",\n",
      "                'Strengthen penalties on fentanyl trafficking.',\n",
      "                'Pass bipartisan privacy legislation to protect our children '\n",
      "                'online--harness the promise of A.I.',\n",
      "                'and protect us from its peril--ban A.I.',\n",
      "                'voice impersonation--and more!',\n",
      "                'And keep our one truly sacred obligation: To train and equip '\n",
      "                \"those we send into harm\\\\\\\\\\\\'s way and care for them and \"\n",
      "                'their families when they come home--and when they '\n",
      "                \"don\\\\\\\\\\\\'t. That\\\\\\\\\\\\'s why I signed the PACT Act--one of \"\n",
      "                'the most significant laws ever--helping millions of veterans '\n",
      "                'who were exposed to toxins and who now are battling more than '\n",
      "                '100 cancers.',\n",
      "                \"Many of them didn\\\\\\\\\\\\'t come home.\",\n",
      "                'We owe them and their families.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 679.2, 504.0, 753.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 70,\n",
      "  'level': 1,\n",
      "  'page_idx': 16,\n",
      "  'sentences': ['And we owe it to ourselves to keep supporting our new health '\n",
      "                'research agency called ARPA-H--and remind us that we can do '\n",
      "                'big things like end cancer as we know it!',\n",
      "                'Let me close with this.',\n",
      "                \"I know I may not look like it--but I\\\\\\\\\\\\'ve been around a \"\n",
      "                'while.',\n",
      "                'And when you get to my age--certain things become clearer '\n",
      "                'than ever before.',\n",
      "                'I know the American story.',\n",
      "                \"Again and again--I\\\\\\\\\\\\'ve seen the contest between \"\n",
      "                'competing forces in the battle for the soul of our Nation.',\n",
      "                'Between those who want to pull America back to the past and '\n",
      "                'those who want to move America into the future.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 212.16, 504.0, 753.36],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 71,\n",
      "  'level': 1,\n",
      "  'page_idx': 17,\n",
      "  'sentences': ['My lifetime has taught me to embrace freedom and democracy.',\n",
      "                'A future based on the core values that have defined America.',\n",
      "                'Honesty.',\n",
      "                'Decency.',\n",
      "                'Dignity.',\n",
      "                'Equality.',\n",
      "                'To respect everyone.',\n",
      "                'To give everyone a fair shot.',\n",
      "                'To give hate no safe harbor.',\n",
      "                'Now--some other people my age see a different story.',\n",
      "                'An American story of resentment--revenge--and retribution.',\n",
      "                \"That\\\\\\\\\\\\'s not me.\",\n",
      "                'I was born amid World War II--when America stood for freedom '\n",
      "                'in the world.',\n",
      "                'I grew up in Scranton, Pennsylvania, and Claymont, Delaware, '\n",
      "                'among working people who built this country.',\n",
      "                'I watched in horror as two of my heroes--Dr.',\n",
      "                'King and Bobby Kennedy-- were assassinated--and their '\n",
      "                'legacies inspired me to pursue a career in service.',\n",
      "                'A public defender--county councilman--elected United States '\n",
      "                'Senator at 29--then Vice President to our first Black '\n",
      "                'President--now President with our first woman Vice President.',\n",
      "                \"In my career--I\\\\\\\\\\\\'ve been told I\\\\\\\\\\\\'m too young--and \"\n",
      "                \"I\\\\\\\\\\\\'m too old.\",\n",
      "                \"Whether young or old--I\\\\\\\\\\\\'ve always known what endures.\",\n",
      "                'Our North Star.',\n",
      "                'The very idea of America--that we are all created equal and '\n",
      "                'deserve to be treated equally throughout our lives.',\n",
      "                \"We\\\\\\\\\\\\'ve never fully lived up to that idea--but \"\n",
      "                \"we\\\\\\\\\\\\'ve never walked away from it either.\",\n",
      "                \"And I won\\\\\\\\\\\\'t walk away from it now. My fellow \"\n",
      "                \"Americans--the issue facing our Nation isn\\\\\\\\\\\\'t how old we \"\n",
      "                \"are--it\\\\\\\\\\\\'s how old are our ideas?\",\n",
      "                'Hate--anger--revenge--retribution-- are among the oldest of '\n",
      "                'ideas.',\n",
      "                \"But you can\\\\\\\\\\\\'t lead America with ancient ideas that only \"\n",
      "                'take us back.',\n",
      "                'To lead America--the land of possibilities--you need a vision '\n",
      "                'for the future of what America can and should be.',\n",
      "                \"Tonight--you\\\\\\\\\\\\'ve heard mine.\",\n",
      "                'I see a future where we defend democracy--not diminish it.',\n",
      "                'I see a future where we restore the right to choose and '\n",
      "                'protect other freedoms--not take them away.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 103.44, 498.0, 130.8],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 72,\n",
      "  'level': 1,\n",
      "  'page_idx': 18,\n",
      "  'sentences': ['I see a future where the middle class finally has a fair '\n",
      "                'shot--and the wealthy finally have to pay their fair share in '\n",
      "                'taxes.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 134.4, 504.0, 162.0],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 73,\n",
      "  'level': 1,\n",
      "  'page_idx': 18,\n",
      "  'sentences': ['I see a future where we save the planet from the climate '\n",
      "                'crisis and our country from gun violence.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 165.6, 498.0, 348.72],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 74,\n",
      "  'level': 1,\n",
      "  'page_idx': 18,\n",
      "  'sentences': ['Above all--I see a future for all Americans!',\n",
      "                'I see a country for all Americans!',\n",
      "                'And I will always be a President for all Americans!',\n",
      "                'Because I believe in America!',\n",
      "                'I believe in you--the American people.',\n",
      "                \"You\\\\\\\\\\\\'re the reason I\\\\\\\\\\\\'ve never been more optimistic \"\n",
      "                'about our future!',\n",
      "                \"So let\\\\\\\\\\\\'s build that future together!\",\n",
      "                \"Let\\\\\\\\\\\\'s remember who we are!\",\n",
      "                'We are the United States of America.',\n",
      "                'There is nothing beyond our capacity when we act together!',\n",
      "                'May God bless you all.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [102.0, 352.32, 264.0, 364.32],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 75,\n",
      "  'level': 1,\n",
      "  'page_idx': 18,\n",
      "  'sentences': ['May God protect our troops.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [384.0, 367.92, 504.0, 379.92],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 76,\n",
      "  'level': 1,\n",
      "  'page_idx': 18,\n",
      "  'sentences': ['Joseph R. Biden, Jr.'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [90.0, 383.52, 276.0, 395.52],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 77,\n",
      "  'level': 1,\n",
      "  'page_idx': 18,\n",
      "  'sentences': ['The White House, March 7, 2024.'],\n",
      "  'tag': 'para'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(doc_file.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bbox': [90.0, 72.24, 511.3, 146.39999999999998],\n",
      " 'block_class': 'cls_0',\n",
      " 'block_idx': 0,\n",
      " 'level': 0,\n",
      " 'page_idx': 0,\n",
      " 'sentences': ['[Congressional Record Volume 170, Number 41 (Thursday, March '\n",
      "               '7, 2024)] [Senate] [Pages S2272-S2277] From the Congressional '\n",
      "               'Record Online through the Government Publishing Office '\n",
      "               '[www.gpo.gov]'],\n",
      " 'tag': 'para'}\n",
      "{'bbox': [108.0, 212.16, 492.0, 317.76],\n",
      " 'block_class': 'cls_0',\n",
      " 'block_idx': 1,\n",
      " 'level': 0,\n",
      " 'page_idx': 0,\n",
      " 'sentences': ['PRESIDENTIAL MESSAGE REPORT ON THE STATE OF THE UNION '\n",
      "               'DELIVERED TO A JOINT SESSION OF CONGRESS ON MARCH 7, 2024--PM '\n",
      "               '41'],\n",
      " 'tag': 'para'}\n",
      "{'bbox': [90.0, 336.72, 498.0, 395.52],\n",
      " 'block_class': 'cls_0',\n",
      " 'block_idx': 2,\n",
      " 'level': 1,\n",
      " 'page_idx': 0,\n",
      " 'sentences': ['The PRESIDING OFFICER laid before the Senate the following '\n",
      "               'message from the President of the United States which was '\n",
      "               'ordered to lie on the table:'],\n",
      " 'tag': 'para'}\n",
      "{'bbox': [90.0, 414.48, 456.0, 457.68],\n",
      " 'block_class': 'cls_0',\n",
      " 'block_idx': 3,\n",
      " 'level': 1,\n",
      " 'page_idx': 0,\n",
      " 'sentences': ['To the Congress of the United States: Good evening.',\n",
      "               'Mr. Speaker.',\n",
      "               'Madam Vice President.',\n",
      "               'Members of Congress.'],\n",
      " 'tag': 'para'}\n",
      "{'bbox': [90.0, 461.28, 210.0, 473.28],\n",
      " 'block_class': 'cls_0',\n",
      " 'block_idx': 4,\n",
      " 'level': 1,\n",
      " 'page_idx': 0,\n",
      " 'sentences': ['My Fellow Americans.'],\n",
      " 'tag': 'para'}\n",
      "{'bbox': [90.0, 476.88, 498.0, 535.44],\n",
      " 'block_class': 'cls_0',\n",
      " 'block_idx': 5,\n",
      " 'level': 1,\n",
      " 'page_idx': 0,\n",
      " 'sentences': ['In January 1941--President Franklin Roosevelt came to this '\n",
      "               'chamber to speak to the Nation.',\n",
      "               'He said---``I address you at a moment unprecedented'],\n",
      " 'tag': 'para'}\n",
      "{'bbox': [90.0, 554.64, 174.0, 566.64],\n",
      " 'block_class': 'cls_0',\n",
      " 'block_idx': 6,\n",
      " 'level': 1,\n",
      " 'page_idx': 0,\n",
      " 'sentences': ['[[Page S2273]]'],\n",
      " 'tag': 'para'}\n",
      "{'bbox': [90.0, 585.84, 498.0, 675.6],\n",
      " 'block_class': 'cls_0',\n",
      " 'block_idx': 7,\n",
      " 'level': 1,\n",
      " 'page_idx': 0,\n",
      " 'sentences': [\"in the history of the Union\\\\\\\\\\\\'\\\\\\\\\\\\'.\",\n",
      "               'Hitler was on the march.',\n",
      "               'War was raging in Europe.',\n",
      "               \"President Roosevelt\\\\\\\\\\\\'s purpose was to wake up the \"\n",
      "               'Congress and alert the American people that this was no '\n",
      "               'ordinary moment.',\n",
      "               'Freedom and democracy were under assault in the world.'],\n",
      " 'tag': 'para'}\n",
      "{'bbox': [90.0, 679.2, 498.0, 753.36],\n",
      " 'block_class': 'cls_0',\n",
      " 'block_idx': 8,\n",
      " 'level': 1,\n",
      " 'page_idx': 0,\n",
      " 'sentences': ['Tonight--I come to the same chamber to address the Nation.',\n",
      "               'Now it is we who face an unprecedented moment in the history '\n",
      "               'of the Union.',\n",
      "               'And yes--my purpose tonight is to both wake up this Congress '\n",
      "               'and alert the American people that this is no ordinary moment '\n",
      "               'either.',\n",
      "               'Not since President Lincoln and the Civil War have freedom and '\n",
      "               'democracy been under assault here at home as they are today.',\n",
      "               'What makes our moment rare is that freedom and democracy are '\n",
      "               'under attack both at home and overseas at the very same time.'],\n",
      " 'tag': 'para'}\n",
      "{'bbox': [90.0, 181.2, 498.0, 411.12],\n",
      " 'block_class': 'cls_0',\n",
      " 'block_idx': 9,\n",
      " 'level': 1,\n",
      " 'page_idx': 1,\n",
      " 'sentences': ['Overseas--Putin of Russia--is on the march.',\n",
      "               'Invading Ukraine--and sowing chaos throughout Europe and '\n",
      "               'beyond.',\n",
      "               'If anybody in this room thinks Putin will stop at Ukraine--I '\n",
      "               'assure you--he will not.',\n",
      "               'But Ukraine can stop Putin--if we stand with Ukraine.',\n",
      "               'And provide the weapons it needs to defend itself.',\n",
      "               'That is all Ukraine is asking.',\n",
      "               'They are not asking for American soldiers.',\n",
      "               'In fact--there are no American soldiers at war in Ukraine.',\n",
      "               'And I am determined to keep it that way.',\n",
      "               'But now--assistance for Ukraine is being blocked by those who '\n",
      "               'want us to walk away from our leadership in the world.',\n",
      "               \"It wasn\\\\\\\\\\\\'t that long ago when a Republican \"\n",
      "               'President--Ronald Reagan--thundered--``Mr.',\n",
      "               \"Gorbachev--tear down this wall\\\\\\\\\\\\'\\\\\\\\\\\\'.\"],\n",
      " 'tag': 'para'}\n",
      "{'bbox': [90.0, 414.48, 492.0, 504.48],\n",
      " 'block_class': 'cls_0',\n",
      " 'block_idx': 10,\n",
      " 'level': 1,\n",
      " 'page_idx': 1,\n",
      " 'sentences': ['Now--my predecessor--a former Republican President--tells '\n",
      "               \"Putin-- ``Do whatever the hell you want.\\\\\\\\\\\\'\\\\\\\\\\\\' A \"\n",
      "               'former American President actually said that--bowing down--to '\n",
      "               'a Russian leader.',\n",
      "               \"It\\\\\\\\\\\\'s outrageous.\",\n",
      "               \"It\\\\\\\\\\\\'s dangerous.\",\n",
      "               \"It\\\\\\\\\\\\'s unacceptable.\"],\n",
      " 'tag': 'para'}\n",
      "{'bbox': [90.0, 507.84, 480.0, 566.6400000000001],\n",
      " 'block_class': 'cls_0',\n",
      " 'block_idx': 11,\n",
      " 'level': 1,\n",
      " 'page_idx': 1,\n",
      " 'sentences': ['America is a founding member of NATO--the military alliance of '\n",
      "               'democratic nations--created after World War II to prevent war '\n",
      "               'and keep the peace.'],\n",
      " 'tag': 'para'}\n"
     ]
    }
   ],
   "source": [
    "listItem = None\n",
    "count = 0\n",
    "for item in doc_file.json:\n",
    "    # print the first item with a tag of list_item\n",
    "    if item[\"tag\"] == \"para\":\n",
    "        pprint(item)\n",
    "        listItem = ListItem(item)\n",
    "        if count > 10:\n",
    "            break\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'para'}\n"
     ]
    }
   ],
   "source": [
    "# find all the unique tags in this document\n",
    "unique_tags = set(item[\"tag\"] for item in doc_file.json)\n",
    "print(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<li>America is a founding member of NATO--the military alliance of democratic nations--created after World War II to prevent war and keep the peace.</li>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(listItem.to_html(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "from PIL import ImageDraw\n",
    "\n",
    "# Set the path to the PDF file\n",
    "pdf_path = \"documents/word.pdf\"\n",
    "\n",
    "# Set the page number and bounding box coordinates\n",
    "page_number = 1  # Example: 1\n",
    "\n",
    "# Convert the PDF page to an image\n",
    "images = convert_from_path(pdf_path, first_page=page_number, last_page=page_number)\n",
    "\n",
    "correction_multiple = 2.78\n",
    "# bbox = (67.85*correction_multiple, 130.82*correction_multiple, 148.02*correction_multiple, 140.79999999999998*correction_multiple)\n",
    "bbox = (108.0, 212.16, 492.0, 317.76)\n",
    "bbox_corrected = tuple(coord * correction_multiple for coord in bbox)\n",
    "\n",
    "\n",
    "# bbox_inches = (67.85, 130.82, 148.02, 140.79999999999998)\n",
    "# bbox_points = tuple()\n",
    "# Crop the image based on the bounding box coordinates\n",
    "# cropped_image = images[0].crop(bbox)\n",
    "page = images[0]\n",
    "\n",
    "# Draw a red bounding box on the image\n",
    "draw = ImageDraw.Draw(page)\n",
    "draw.rectangle(bbox_corrected, outline=\"red\")\n",
    "\n",
    "# Save the cropped image with the bounding box\n",
    "page.save(\"screenshot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = doc_file.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'bbox': [90.0, 72.24, 511.3, 146.39999999999998],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 0,\n",
      "  'level': 0,\n",
      "  'page_idx': 0,\n",
      "  'sentences': ['[Congressional Record Volume 170, Number 41 (Thursday, March '\n",
      "                '7, 2024)] [Senate] [Pages S2272-S2277] From the Congressional '\n",
      "                'Record Online through the Government Publishing Office '\n",
      "                '[www.gpo.gov]'],\n",
      "  'tag': 'para'},\n",
      " {'bbox': [108.0, 212.16, 492.0, 317.76],\n",
      "  'block_class': 'cls_0',\n",
      "  'block_idx': 1,\n",
      "  'level': 0,\n",
      "  'page_idx': 0,\n",
      "  'sentences': ['PRESIDENTIAL MESSAGE REPORT ON THE STATE OF THE UNION '\n",
      "                'DELIVERED TO A JOINT SESSION OF CONGRESS ON MARCH 7, 2024--PM '\n",
      "                '41'],\n",
      "  'tag': 'para'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(json_file[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cls_0'}\n"
     ]
    }
   ],
   "source": [
    "# find all the unique block_class\n",
    "unique_block_classes = set(item[\"block_class\"] for item in json_file)\n",
    "print(unique_block_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "# print all levels of the json file\n",
    "unique_levels = set(item[\"level\"] for item in json_file)\n",
    "print(unique_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = LayoutReader()\n",
    "layout_tree = temp.read(doc_file.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The PRESIDING OFFICER laid before the Senate the following message from the President of the United States which was ordered to lie on the table:\n",
      "\n",
      "Context to text: \n",
      "The PRESIDING OFFICER laid before the Senate the following message from the President of the United States which was ordered to lie on the table:\n"
     ]
    }
   ],
   "source": [
    "print(\"Text:\", layout_tree.children[2].to_text())\n",
    "print()\n",
    "print(\"Context to text:\", layout_tree.children[2].to_context_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'para'}\n"
     ]
    }
   ],
   "source": [
    "unique_tags = set(item[\"tag\"] for item in json_file)\n",
    "print(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Congressional Record Volume 170, Number 41 (Thursday, March 7, 2024)] [Senate] [Pages S2272-S2277] From the Congressional Record Online through the Government Publishing Office [www.gpo.gov]\n",
      "PRESIDENTIAL MESSAGE REPORT ON THE STATE OF THE UNION DELIVERED TO A JOINT SESSION OF CONGRESS ON MARCH 7, 2024--PM 41\n"
     ]
    }
   ],
   "source": [
    "for child in layout_tree.children:\n",
    "    if child.level == 0:\n",
    "        print (child.to_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_block_tree(root, graph=None):\n",
    "    if graph is None:\n",
    "        graph = Digraph()\n",
    "    \n",
    "    node_id = str(id(root))\n",
    "    graph.node(node_id, f\"{root.tag} (level {root.level})\")\n",
    "\n",
    "    for child in root.children:\n",
    "        child_id = str(id(child))\n",
    "        graph.node(child_id, f\"{child.tag} (level {child.level})\")\n",
    "        graph.edge(node_id, child_id)\n",
    "        # reset the root node and recurse\n",
    "        visualize_block_tree(child, graph)\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_tree = temp.read(doc_url.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Useful!] Helps us visualise the layout tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'block_tree.pdf'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = visualize_block_tree(layout_tree)\n",
    "graph.render('block_tree', view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of top level headers:\n",
      "3\n",
      "\n",
      "Top level headers right after root:\n",
      "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
      "8 Conclusions\n",
      "References\n"
     ]
    }
   ],
   "source": [
    "# identify the headers for this current level. There's many more headers.\n",
    "headers = [block for block in layout_tree.children if block.tag == \"header\"]\n",
    "print(\"Number of top level headers:\")\n",
    "print(len(headers))\n",
    "print()\n",
    "print(\"Top level headers right after root:\")\n",
    "for header in headers:\n",
    "    print(header.to_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
      "Mike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n",
      "{mikelewis,yinhanliu,naman}@fb.com\n"
     ]
    }
   ],
   "source": [
    "print(headers[0].to_text(include_children=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llmsherpa.readers.layout_reader.Paragraph object at 0x114effdd0>, <llmsherpa.readers.layout_reader.Section object at 0x114efd550>]\n"
     ]
    }
   ],
   "source": [
    "print(headers[0].children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "para\n",
      "Mike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n",
      "header\n",
      "{mikelewis,yinhanliu,naman}@fb.com\n"
     ]
    }
   ],
   "source": [
    "for child in headers[0].children:\n",
    "    print(child.tag)\n",
    "    print(child.to_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reader identified \"BART...\" as a top-level header (level 0), that links to its two children \"Mike Lewis...\" as a paragraph (level 0), that links to \"{mikelewis ...}...\" as a second-level header (level 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data/header-demo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract\n",
      "1 Introduction\n",
      "2 Model\n",
      "3 Fine-tuning BART\n",
      "4 Comparing Pre-training Objectives\n",
      "5 Large-scale Pre-training Experiments\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    print(headers[0].children[1].children[i].to_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above, the reader is able to indentify the respective headers of this pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deepest Child\n",
      "Recent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n",
      "To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n",
      "\n",
      "First Header\n",
      "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
      "\n",
      "Entire chain\n",
      "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
      "Mike Lewis*, Yinhan Liu*, Naman Goyal*, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer Facebook AI\n",
      "{mikelewis,yinhanliu,naman}@fb.com\n",
      "Abstract\n",
      "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n",
      "BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n",
      "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n",
      "We evaluate a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the original sentences and using a novel in-ﬁlling scheme, where spans of text are replaced with a single mask token.\n",
      "BART is particularly effective when ﬁne tuned for text generation but also works well for comprehension tasks.\n",
      "It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n",
      "BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n",
      "We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most inﬂuence end-task performance.\n",
      "1 Introduction\n",
      "methods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019).\n",
      "The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out.\n",
      "Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019).\n",
      "However, these methods typically focus on particular types of end tasks (e.g. span prediction, generation, etc.), limiting their applicability.\n",
      "In this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers.\n",
      "BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks.\n",
      "Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text.\n",
      "BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1).\n",
      "A key advantage of this setup is the noising ﬂexibility; arbitrary transformations can be applied to the original text, including changing its length.\n",
      "We evaluate a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the original sentences and using a novel in-ﬁlling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.\n",
      "This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.\n",
      "BART is particularly effective when ﬁne tuned for text generation but also works well for comprehension tasks.\n",
      "It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks.\n",
      "For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018).\n",
      "BART also opens up new ways of thinking about ﬁne tuning.\n",
      "We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.\n",
      "These layers are trained to essentially translate the foreign language to noised\n",
      "B D A B C D E\n",
      "Autoregressive Decoder\n",
      "Bidirectional Encoder\n",
      "A _ C _ E\n",
      "<s> A B C D\n",
      "(a) BERT: Random tokens are replaced with masks, and the document is encoded bidirectionally.\n",
      "Missing tokens are predicted independently, so BERT cannot easily be (b) GPT: Tokens are predicted auto-regressively, meaning GPT can be used for generation.\n",
      "However words can only condition on leftward context, so it cannot learn bidirec- tional interactions.\n",
      "used for generation.\n",
      "A B C D E\n",
      " | Bidirectional Encoder | Autoregressive Decoder\n",
      " | A _ B _ E <s> A B C D\n",
      "\n",
      "(c) BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations.\n",
      "Here, a document has been corrupted by replacing spans of text with mask symbols.\n",
      "The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder.\n",
      "For ﬁne-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the ﬁnal hidden state of the decoder.\n",
      "Figure 1: A schematic comparison of BART with BERT (Devlin et al., 2019) and GPT (Radford et al., 2018).\n",
      "English, by propagation through BART, thereby using BART as a pre-trained target-side language model.\n",
      "This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark.\n",
      "To better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives.\n",
      "This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019).\n",
      "We ﬁnd that BART exhibits the most consistently strong performance across the full range of tasks we consider.\n",
      "2 Model\n",
      "is a denoising autoencoder that maps a corrupted document to the original document it was derived from.\n",
      "It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder.\n",
      "For pre-training, we optimize the negative log likelihood of the original document.\n",
      "2.1 Architecture\n",
      "BART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).\n",
      "For our base model, we use 6 layers in the encoder and de- coder, and for our large model we use 12 layers in each.\n",
      "The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the ﬁnal hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not.\n",
      "In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.\n",
      "2.2 Pre-training BART\n",
      "BART is trained by corrupting documents and then optimizing a reconstruction loss—the cross-entropy between the decoder’s output and the original document.\n",
      "Unlike existing denoising autoencoders, which are tailored to speciﬁc noising schemes, BART allows us to apply any type of document corruption.\n",
      "In the extreme case, where all information about the source is lost, BART is equivalent to a language model.\n",
      "We experiment with several previously proposed and novel transformations, but we believe there is a signiﬁcant potential for development of other new alternatives.\n",
      "The transformations we used are summarized below, and examples are shown in Figure 2.\n",
      "Token Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements.\n",
      "Token Deletion Random tokens are deleted from the input.\n",
      "In contrast to token masking, the model must decide which positions are missing inputs.\n",
      " | A _C . _ E . | D E . A B C . | C . D E . A B\n",
      " | --- | --- | ---\n",
      " |  | Sentence Permutation | Document RotationToken Masking\n",
      " |  | A B C . D E .A . C . E . | A _ . D _ E .\n",
      "\n",
      "Token Deletion Text Inﬁlling\n",
      "Figure 2: Transformations for noising the input that we experiment with.\n",
      "These transformations can be composed.\n",
      "Text Inﬁlling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (λ = 3).\n",
      "Each span is replaced with a single [MASK] token.\n",
      "0-length spans correspond to the insertion of [MASK] tokens.\n",
      "Text inﬁlling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length.\n",
      "Text inﬁlling teaches the model to predict how many tokens are missing from a span.\n",
      "Sentence Permutation A document is divided into sentences based on full stops, and these sentences are shufﬂed in a random order.\n",
      "Document Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token.\n",
      "This task trains the model to identify the start of the document.\n",
      "3 Fine-tuning BART\n",
      "The representations produced by BART can be used in several ways for downstream applications.\n",
      "3.1 Sequence Classiﬁcation Tasks\n",
      "For sequence classiﬁcation tasks, the same input is fed into the encoder and decoder, and the ﬁnal hidden state of the ﬁnal decoder token is fed into new multi-class linear classiﬁer.\n",
      "This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a).\n",
      "3.2 Token Classiﬁcation Tasks\n",
      "For token classiﬁcation tasks, such as answer endpoint classiﬁcation for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.\n",
      "This representation is used to classify the token.\n",
      "3.3 Sequence Generation Tasks\n",
      "Because BART has an autoregressive decoder, it can be directly ﬁne tuned for sequence generation tasks such as abstractive question answering and summarization.\n",
      "In both of these tasks, information is copied from the input but manipulated, which is closely related to the denoising pre-training objective.\n",
      "Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively.\n",
      "3.4 Machine Translation\n",
      "We also explore using BART to improve machine translation decoders for translating into English.\n",
      "Previous work Edunov et al.\n",
      "(2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited.\n",
      "We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b).\n",
      "More precisely, we replace BART’s encoder embedding layer with a new randomly initialized encoder.\n",
      "The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English.\n",
      "The new encoder can use a separate vocabulary from the original BART model.\n",
      "We train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model.\n",
      "In the ﬁrst step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART’s encoder ﬁrst layer.\n",
      "In the second step, we train all model parameters for a small number of iterations.\n",
      "4 Comparing Pre-training Objectives\n",
      "BART supports a much wider range of noising schemes during pre-training than previous work.\n",
      "We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in §5.\n",
      "4.1 Comparison Objectives\n",
      "While many pre-training objectives have been proposed, fair comparisons between these have been difﬁcult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and ﬁne-tuning procedures.\n",
      "We\n",
      "A\n",
      "B\n",
      "C\n",
      "D E\n",
      "label\n",
      " | Pre-trained Encoder | Pre-trained Decoder\n",
      " | --- | ---\n",
      " | Pre-trained Encoder | Pre-trained Decoder\n",
      "\n",
      " | <s> A | B | C | D\n",
      " | --- | --- | --- | ---\n",
      " | Randomly Initialized Encoder\n",
      "\n",
      "A B C D E <s> A B C D E\n",
      "α\n",
      "β\n",
      "γ\n",
      "δ ε\n",
      "(a) To use BART for classiﬁcation problems, the same input is fed into the encoder and decoder, and the repre- sentation from the ﬁnal output is used.\n",
      "(b) For machine translation, we learn a small additional encoder that replaces the word embeddings in BART.\n",
      "The new encoder can use a disjoint vocabulary.\n",
      "Figure 3: Fine tuning BART for classiﬁcation and translation.\n",
      "re-implement strong pre-training approaches recently proposed for discriminative and generation tasks.\n",
      "We aim, as much as possible, to control for differences unrelated to the pre-training objective.\n",
      "However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective).\n",
      "For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.\n",
      "We compare the following approaches:\n",
      "Language Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model.\n",
      "This model is equivalent to the BART decoder, without cross-attention.\n",
      "Permuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.\n",
      "For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet.\n",
      "Masked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens.\n",
      "Multitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks.\n",
      "Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the ﬁrst 50% of tokens unmasked and a left-to-right mask for the remainder.\n",
      "Masked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens.\n",
      "For the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to efﬁciently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right).\n",
      "We experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as preﬁx to the target in the decoder, with a loss only on the target part of the sequence.\n",
      "We ﬁnd the former works better for BART models, and the latter for other models.\n",
      "To most directly compare our models on their ability to model their ﬁne-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n",
      "4.2 Tasks\n",
      "SQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs.\n",
      "Answers are text spans extracted from a given document context.\n",
      "Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder.\n",
      "The model includes classiﬁers to predict the start and end indices of each token.\n",
      "MNLI (Williams et al., 2017), a bitext classiﬁcation task to predict whether one sentence entails another.\n",
      "The ﬁne-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder.\n",
      "In contrast to BERT, the representation of the EOS token is used to classify the sentences relations.\n",
      "ELI5 (Fan et al., 2019), a long-form abstractive question answering dataset.\n",
      "Models generate answers conditioned on the concatenation of a question and supporting documents.\n",
      "XSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries.\n",
      "ConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona.\n",
      "CNN/DM (Hermann et al., 2015), a news summarization dataset.\n",
      "Summaries here are typically closely related to source sentences.\n",
      "4.3 Results\n",
      "Results are shown in Table 1.\n",
      "Several trends are clear:\n",
      " | Model | SQuAD 1.1 | MNLI | ELI5 | XSum | ConvAI2 | CNN/DM\n",
      " | --- | --- | --- | --- | --- | --- | ---\n",
      " | F1 Acc PPL PPL PPL PPL\n",
      " | BERT Base (Devlin et al., 2019) | 88.5 | 84.3 | - | - | - | -\n",
      " | Masked Language Model | 90.0 | 83.5 | 24.77 | 7.87 | 12.59 | 7.06\n",
      " | Masked Seq2seq | 87.0 | 82.1 | 23.40 | 6.80 | 11.43 | 6.19\n",
      " | Language Model | 76.7 | 80.1 | 21.40 | 7.00 | 11.51 | 6.56\n",
      " | Permuted Language Model | 89.1 | 83.7 | 24.03 | 7.69 | 12.23 | 6.96\n",
      " | Multitask Masked Language Model | 89.2 | 82.4 | 23.73 | 7.50 | 12.39 | 6.74\n",
      " | BART Base w/ Token Masking | 90.4 | 84.1 | 25.05 | 7.08 | 11.73 | 6.10\n",
      " | w/ Token Deletion | 90.4 | 84.1 | 24.61 | 6.90 | 11.46 | 5.87\n",
      " | w/ Text Inﬁlling | 90.8 | 84.0 | 24.26 | 6.61 | 11.05 | 5.83\n",
      " | w/ Document Rotation | 77.2 | 75.3 | 53.69 | 17.14 | 19.87 | 10.59\n",
      " | w/ Sentence Shufﬂing | 85.4 | 81.5 | 41.87 | 10.93 | 16.67 | 7.89\n",
      " | w/ Text Inﬁlling + Sentence Shufﬂing | 90.8 | 83.8 | 24.17 | 6.62 | 11.12 | 5.41\n",
      "\n",
      "Table 1: Comparison of pre-training objectives.\n",
      "All models are of comparable size and are trained for 1M steps on a combination of books and Wikipedia data.\n",
      "Entries in the bottom two blocks are trained on identical data using the same code-base, and ﬁne-tuned with the same procedures.\n",
      "Entries in the second block are inspired by pre-training objectives proposed in previous work, but have been simpliﬁed to focus on evaluation objectives (see §4.1).\n",
      "Performance varies considerably across tasks, but the BART models with text inﬁlling demonstrate the most consistently strong performance.\n",
      "Performance of pre-training methods varies signiﬁcantly across tasks The effectiveness of pre-training methods is highly dependent on the task.\n",
      "For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results.\n",
      "Token masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation.\n",
      "The successful methods either use token deletion or masking, or self-attention masks.\n",
      "Deletion appears to outperform masking on generation tasks.\n",
      "Left-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training.\n",
      "Bidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classiﬁcation decisions.\n",
      "However, BART achieves similar performance with only half the number of bidirectional layers.\n",
      "The pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019).\n",
      "Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence.\n",
      "Pure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART.\n",
      "A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input.\n",
      "BART achieves the most consistently strong performance.\n",
      "With the exception of ELI5, BART models using text-inﬁlling perform well on all tasks.\n",
      "5 Large-scale Pre-training Experiments\n",
      "Recent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora.\n",
      "To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model.\n",
      "5.1 Experimental Setup\n",
      "We pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024.\n",
      "Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps.\n",
      "Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019).\n",
      "Based on the results in Section §4, we use a combination of text inﬁlling and sentence permutation.\n",
      "We mask 30% of tokens in each document, and permute all sentences.\n",
      "Although sentence permutation only shows signiﬁcant additive gains\n",
      " |  | SQuAD 1.1 EM/F1 | SQuAD 2.0 EM/F1 | MNLI m/mm | SST Acc | QQP Acc | QNLI Acc | STS-B Acc | RTE Acc | MRPC Acc | CoLA Mcc\n",
      " | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n",
      " | BERT | 84.1/90.9 | 79.0/81.8 | 86.6/- | 93.2 | 91.3 | 92.3 | 90.0 | 70.4 | 88.0 | 60.6\n",
      " | UniLM | -/- | 80.5/83.4 | 87.0/85.9 | 94.5 | - | 92.7 | - | 70.9 | - | 61.1\n",
      " | XLNet | 89.0/94.5 | 86.1/88.8 | 89.8/- | 95.6 | 91.8 | 93.9 | 91.8 | 83.8 | 89.2 | 63.6\n",
      " | RoBERTa | 88.9/94.6 | 86.5/89.4 | 90.2/90.2 | 96.4 | 92.2 | 94.7 | 92.4 | 86.6 | 90.9 | 68.0\n",
      " | BART | 88.8/94.6 | 86.1/89.2 | 89.9/90.1 | 96.6 | 92.5 | 94.9 | 91.2 | 87.0 | 90.4 | 62.8\n",
      "\n",
      "Table 2: Results for large models on SQuAD and GLUE tasks.\n",
      "BART performs comparably to RoBERTa and XLNet, suggesting that BART’s uni-directional decoder layers do not reduce performance on discriminative tasks.\n",
      " |  | CNN/DailyMail | XSum\n",
      " | --- | --- | ---\n",
      " |  | R1 | R2 | RL | R1 | R2 | RL\n",
      " | --- | --- | --- | --- | --- | --- | ---\n",
      " | Lead-3 | 40.42 | 17.62 | 36.67 | 16.30 | 1.60 | 11.95\n",
      " | PTGEN (See et al., 2017) | 36.44 | 15.66 | 33.42 | 29.70 | 9.21 | 23.24\n",
      " | PTGEN+COV (See et al., 2017) | 39.53 | 17.28 | 36.38 | 28.10 | 8.02 | 21.72\n",
      " | UniLM | 43.33 | 20.21 | 40.51 | - | - | -\n",
      " | BERTSUMABS (Liu & Lapata, 2019) | 41.72 | 19.39 | 38.76 | 38.76 | 16.33 | 31.15\n",
      " | BERTSUMEXTABS (Liu & Lapata, 2019) | 42.13 | 19.60 | 39.18 | 38.81 | 16.50 | 31.27\n",
      " | BART | 44.16 | 21.28 | 40.90 | 45.14 | 22.27 | 37.25\n",
      "\n",
      "Table 3: Results on two standard summarization datasets.\n",
      "BART outperforms previous work on summarization on two tasks and all metrics, with gains of roughly 6 points on the more abstractive dataset.\n",
      "on the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task.\n",
      "To help the model better ﬁt the data, we disabled dropout for the ﬁnal 10% of training steps.\n",
      "We use the same pre-training data as Liu et al.\n",
      "(2019), consisting of 160Gb of news, books, stories, and web text.\n",
      "5.2 Discriminative Tasks\n",
      "Table 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011).\n",
      "The most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective.\n",
      "Overall, BART performs similarly, with only small differences between the models on most tasks.\n",
      "suggesting that BART’s improvements on generation tasks do not come at the expense of classiﬁcation performance.\n",
      "5.3 Generation Tasks\n",
      "We also experiment with several text generation tasks.\n",
      "BART is ﬁne-tuned as a standard sequence-to-sequence model from the input to the output text.\n",
      "During ﬁnetuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1.\n",
      "During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017).\n",
      " |  | ConvAI2 | Valid F1 Valid PPL\n",
      " | --- | --- | ---\n",
      " | Seq2Seq + Attention | 16.02 | 35.07\n",
      " | --- | --- | ---\n",
      " | Best System | 19.09 | 17.51\n",
      " | BART | 20.72 | 11.85\n",
      "\n",
      "Table 4: BART outperforms previous work on conversational response generation.\n",
      "Perplexities are renormalized based on ofﬁcial tokenizer for ConvAI2.\n",
      "Summarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties.\n",
      "Summaries in the CNN/DailyMail tend to resemble source sentences.\n",
      "Extractive models do well here, and even the baseline of the ﬁrst-three source sentences is highly competitive.\n",
      "Nevertheless, BART outperforms all existing work.\n",
      "In contrast, XSum is highly abstractive, and extractive models perform poorly.\n",
      "BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics—representing a signiﬁcant advance in performance on this problem.\n",
      "Qualitatively, sample quality is high (see §6).\n",
      "Dialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-speciﬁed persona.\n",
      "BART outperforms previous work on two automated metrics.\n",
      "R2 RL\n",
      " | Best Extractive | 23.5 | 3.1 | 17.5\n",
      " | --- | --- | --- | ---\n",
      " | Language Model Seq2Seq | 27.8 28.3 | 4.7 5.1 | 23.1 22.8\n",
      " | Seq2Seq Multitask BART | 28.9 30.6 | 5.4 6.2 | 23.1 24.3\n",
      "\n",
      "Table 5: BART achieves state-of-the-art results on the challenging ELI5 abstractive question answering dataset.\n",
      "Comparison models are from Fan et al.\n",
      "(2019).\n",
      "RO-EN\n",
      "\n",
      " | 7 Related Work | \n",
      " | --- | ---\n",
      " | Baseline | 36.80\n",
      " | Fixed BART 36.29 Tuned BART 37.96\n",
      " | Table 6: The performance (BLEU) of baseline and BART on WMT’16 RO-EN augmented with backtranslation data. BART improves over a strong backtranslation (BT) baseline by using monolingual English pre-training.\n",
      " | Abstractive QA We use the recently proposed ELI5 dataset to test the model’s ability to generate long freeform answers. We ﬁnd BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly speciﬁed by the question.\n",
      " | 5.4 Translation\n",
      " | We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. (2016). We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in §3.4. Experiment results are presented in Table 6. We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). We show the performance of both steps of our model in the ﬁxed BART and tuned BART rows. For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. We use a beam width of 5 and a length penalty of α = 1. Preliminary results suggested that our approach was less effective without back-translation data, and prone to overﬁtting—future work should explore additional regularization techniques.\n",
      " | 6 Qualitative Analysis\n",
      " | BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. To understand BART’s performance beyond automated metrics, we analyse its generations qualitatively.\n",
      " | Table 7 shows example summaries generated by BART. Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model’s training data. Following Narayan et al. (2018), we remove the ﬁrst sentence of the article prior to summarizing it, so there is no easy extractive summary of the document.\n",
      " | Unsurprisingly, model output is ﬂuent and grammatical English. However, model output is also highly abstractive, with few phrases copied from the input. The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). In the ﬁrst example, inferring that ﬁsh are protecting reefs from global warming requires non-trivial inference from the text. However, the claim that the work was published in Science is not supported by the source.\n",
      " | These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation.\n",
      " | Early methods for pretraining were based on language models. GPT (Radford et al., 2018) only models left- ward context, which is problematic for some tasks. ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train inter- actions between these features. Radford et al. (2019) demonstrated that very large language models can act as unsupervised multitask models.\n",
      " | BERT (Devlin et al., 2019) introduced masked lan- guage modelling, which allows pre-training to learn in- teractions between left and right context words. Re- cent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). Predictions are not made auto-regressively, re- ducing the effectiveness of BERT for generation tasks.\n",
      " | UniLM (Dong et al., 2019) ﬁne-tunes BERT with an ensemble of masks, some of which allow only leftward context. Like BART, this allows UniLM to be used for both generative and discriminative tasks. A difference is that UniLM predictions are conditionally indepen- dent, whereas BART’s are autoregressive. BART re- duces the mismatch between pre-training and genera- tion tasks, because the decoder is always trained on un- corrupted context.\n",
      " | MASS (Song et al., 2019) is perhaps the most similar model to BART. An input sequence where a contiguous span of tokens is masked is mapped to a sequence con- sisting of the missing tokens. MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder.\n",
      " | XL-Net (Yang et al., 2019) extends BERT by pre-\n",
      " | Source Document (abbreviated) | BART Summary\n",
      " | The researchers examined three types of coral in reefs off the coast of Fiji The researchers found when ﬁsh were plentiful, they would eat algae and seaweed off the corals, which appeared to leave them more resistant to the bacterium Vibrio coralliilyti- cus, a bacterium associated with bleaching. The researchers sug- gested the algae, like warming temperatures, might render the corals’ chemical defenses less effective, and the ﬁsh were pro- tecting the coral by removing the algae. | Fisheries off the coast of Fiji are protect- ing coral reefs from the effects of global warming, according to a study in the jour- nal Science.\n",
      " | Sacoolas, who has immunity as a diplomat’s wife, was involved in a trafﬁc collision Prime Minister Johnson was questioned about the case while speaking to the press at a hospital in Wat- ford. He said, “I hope that Anne Sacoolas will come back if we can’t resolve it then of course I will be raising it myself personally with the White House.” | Boris Johnson has said he will raise the is- sue of US diplomat Anne Sacoolas’ diplo- matic immunity with the White House.\n",
      " | According to Syrian state media, government forces began de- ploying into previously SDF controlled territory yesterday. On October 6, US President Donald Trump and Turkish Presi- dent Recep Tayyip Erdoan spoke on the phone. Then both na- tions issued statements speaking of an imminent incursion into northeast Syria . On Wednesday, Turkey began a military offensive with airstrikes followed by a ground invasion. Syrian government forces have entered territory held by the US-backed Syrian Democratic Forces (SDF) in response to Turkey’s incursion into the region. | \n",
      " | This is the ﬁrst time anyone has been recorded to run a full marathon of 42.195 kilometers (approximately 26 miles) under this pursued landmark time. It was not, however, an ofﬁcially sanctioned world record, as it was not an”open race” of the IAAF. His time was 1 hour 59 minutes 40.2 seconds. Kipchoge ran in Vienna, Austria. It was an event speciﬁcally designed to help Kipchoge break the two hour barrier. | Kenyan runner Eliud Kipchoge has run a marathon in less than two hours.\n",
      " | PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildﬁres. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow. | Power has been turned off to millions of customers in California as part of a power shutoff plan.\n",
      "\n",
      "Table 7: Example summaries from the XSum-tuned BART model on WikiNews articles.\n",
      "For clarity, only relevant excerpts of the source are shown.\n",
      "Summaries combine information from across the article and prior knowledge.\n",
      "dicting masked tokens auto-regressively in a permuted order.\n",
      "This objective allows predictions to condition on both left and right context.\n",
      "In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation.\n",
      "Several papers have explored using pre-trained representations to improve machine translation.\n",
      "The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest.\n",
      "Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited.\n",
      "We show how BART can be used to improve machine translation decoders.\n"
     ]
    }
   ],
   "source": [
    "# find the deepest child of the first child of the first header\n",
    "deepest_child = headers[0].children[0].children[5].children[0]\n",
    "print(\"Deepest Child\")\n",
    "print(deepest_child.to_text())\n",
    "print()\n",
    "print(\"First Header\")\n",
    "print(headers[0].to_text())\n",
    "# print the entire chain\n",
    "print()\n",
    "print(\"Entire chain\")\n",
    "print(headers[0].to_text(include_children=True, recurse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header 1\n",
      "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension\n",
      "\n",
      "Header 1-1\n",
      "Type: header\n",
      "{mikelewis,yinhanliu,naman}@fb.com\n",
      "\n",
      "Header 1-1-1\n",
      "Type: header\n",
      "Abstract\n",
      "\n",
      "Header 1-1-1-1\n",
      "Type: para\n",
      "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n",
      "BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n",
      "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n",
      "We evaluate a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the original sentences and using a novel in-ﬁlling scheme, where spans of text are replaced with a single mask token.\n",
      "BART is particularly effective when ﬁne tuned for text generation but also works well for comprehension tasks.\n",
      "It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n",
      "BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n",
      "We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most inﬂuence end-task performance.\n"
     ]
    }
   ],
   "source": [
    "print(\"Header 1\")\n",
    "print(headers[0].to_text())\n",
    "print()\n",
    "print(\"Header 1-1\")\n",
    "print(\"Type:\", headers[0].children[1].tag)\n",
    "print(headers[0].children[1].to_text())\n",
    "print()\n",
    "print(\"Header 1-1-1\")\n",
    "print(\"Type:\", headers[0].children[1].children[0].tag)\n",
    "print(headers[0].children[1].children[0].to_text())\n",
    "print()\n",
    "print(\"Header 1-1-1-1\")\n",
    "print(\"Type:\", headers[0].children[1].children[0].children[0].tag)\n",
    "print(headers[0].children[1].children[0].children[0].to_text())\n",
    "# so headers[0].children[1].children[0].children[0] is our first deepest child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\n"
     ]
    }
   ],
   "source": [
    "print(headers[0].children[1].children[0].children[0].parent_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Important!] Verdict: Instead of splitting from top-down, chunk the document using this tree structure, where we only add to the vector database the content of the leaf nodes + the parent_text as seen in the previous code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension > {mikelewis,yinhanliu,naman}@fb.com > Abstract\n",
      "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models.\n",
      "BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n",
      "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes.\n",
      "We evaluate a number of noising approaches, ﬁnding the best performance by both randomly shufﬂing the order of the original sentences and using a novel in-ﬁlling scheme, where spans of text are replaced with a single mask token.\n",
      "BART is particularly effective when ﬁne tuned for text generation but also works well for comprehension tasks.\n",
      "It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE.\n",
      "BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.\n",
      "We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most inﬂuence end-task performance.\n"
     ]
    }
   ],
   "source": [
    "print(headers[0].children[1].children[0].children[0].to_context_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Most important!] Final note: Use the `to_context_text()` function of a node to output the content of a leaf node. Only take information from all leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
