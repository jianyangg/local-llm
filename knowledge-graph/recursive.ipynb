{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way humans think is by having something like a \"mental model\" of the world with networks of concepts and relationships between them. This is a way to represent knowledge and reason about it.\n",
    "Instead of being able to see the big picture of network, we eagerly explore from one node to another, following the links between them.\n",
    "This way of querying the network is more simple and straightforward rather than trying to identify a specific query to explore the network.\n",
    "Following this idea, it makes our LLM-generated cypher queries more accurate and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data into neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('documents/Adyen_ A First Principles Payment Platform.pdf')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"Introduction\\n[00:01:59]Zack: This is Zack Fuss and today we are breaking  down European  based pay business , Adyen. Adyen was found \\nin Amsterdam  in 2006 by a group of payments  entrepreneurs  who had already built and sold a business  in the space. Adyen \\nwas their chance to start a fresh and build a modern solution  to displace  the patchwork  legacy system that merchants  were \\nbeing forced to use. To breakdown  the business , I'm joined by Michael Willar, a portfolio  manager  at Stenham  Asset \\nManagement . Our discussion  covers Adyen's single platform  solution  in detail, the driving force behind their track record of \\nprofitable  growth, and why payments  isn't a winner take all market. Please enjoy this breakdown  of Adyen. A Bird's Eye View of Adyen and Payments\\n[00:02:42]Zack: So today we'll be breaking  down Adyen, a large payment  processing  business . Despite its nearly $70 billion \\nUS market cap, it's a business  that's relatively  unknown  by American  investors .\"),\n",
      " Document(page_content='I thought a great place to start would be just \\nbriefly explain the business  from a thousand  feet. [00:03:02]Michael: Yeah, absolutely . And thanks guys for having me on. Adyen is a payment  processor  that came to the \\nmarkets  to change not only the rules of payment  processing , but the entire game itself. Today, they count the largest and \\nfastest growing enterprises  in the world as customers  and across multiple  different  industries . So think Microsoft , Overmatch , \\nMcDonald\\'s , Nike, just to name a few. So you might not be familiar with Adyen because  they\\'re not consumer  facing, but they \\nare likely familiar with you and one of your transactions . So Adyen is a Dutch business  with headquarters  in Amsterdam . It\\'s \\ntruly global with 27 offices around the world. Their net revenue split today is 60% Europe, about 24% in the US and APAC and \\nLatin America  both high single digits. They currently  have about 2,200 full-time employees , which is incredibly  small for their \\nage and scale, which I\\'m sure we\\'ll get into, and have over a hundred  different  nationalities . This is all by design. But the best way to think about Adyen is they exist to solve problems  for merchants . They are a bunch of \\npayment  nerds who get excited about complexity . They go into work every single day and say, \"How can we help our \\nmerchants  generate  more incremental  revenue?\" And I think this is a good purpose  to have because  there are so many \\nproblems  in payments  today where 20% of all online transactions  are declined . And this is versus 5% in the offline world. And \\nthey are able to effectively  solve these problems  because  they built a single platform  all in house where they directly connect  \\nthe card networks . This in essence  is what makes Adyen unique relative to the payment  processing  industry. They\\'re what you \\ncall a full stack acquirer  where they\\'ve obtained  a lot of the relevant local acquiring  licenses  around the world and now have \\ntwo banking licenses  in EU and the US. And this gives merchants  high probability  of higher authorization  rates, which is just the currency  in enterprise  processing , \\nlower transaction  costs, and faster settlement . But their single platform  also means a merchant  can scale into most major \\nmarkets  around the world where the turnaround  is weeks or months, not years like some competitors . They\\'ll be able to \\naccept nearly 300 local payment  methods . You\\'ll be able to sell in any channel, all the while having access to transaction  data \\nacross the payments  chain. And this is again, totally different  to how the industry  does things. They wanted to redefine  the \\nentire industry. So no other payment  process in the world today can truly say they have one single platform  where they don\\'t \\noutsource  anything  and control the entire stack end to end. [00:05:51]Zack: That\\'s fantastic  context.'),\n",
      " Document(page_content=\"So before we dive deeper into Adyen, it'd be helpful if you could just break down \\nhow payments  in general work. Â® 2024 Colossus, LLC. All rights reserved.Colossus, LLC.\"),\n",
      " Document(page_content=\"[00:06:00]Michael: Have you ever wondered  what happens  when you tap your phone or card for your Monday morning  coffee \\nand in a few milliseconds  it's been processed , authorized , and debited from your bank account ? Perhaps  you haven't thought \\nabout it too much because  it just works, most of the time at least. And it's generally  a frictionless  consumer  experience . But \\nit's actually quite incredible , or at least I think it is. Because  what happens  in those milliseconds  is fairly convoluted  and \\ninvolves  a number of key players all performing  different  tasks. So a standard  card based payment  will have a few key parties \\ninvolved . The first party will be you, the customer , paying for something . Then you have the merchants  who accepts your \\nmoney with a point of sale terminal , in the offline world, or maybe a gateway  in the online world. Then the issuing bank, who \\nwill be the customers  bank, and they issue a debit or credit card. So think Barclays  here in the UK or JP Morgan or Bank of America . Then you have the card networks  who provide the \\ninfrastructure  for the transaction  to happen on. They are effectively  the gatekeepers  of payments  and literally make the rules \\nfor participants , meaning  they set the interchange  fees and you need a license to operate on their network . The two largest are \\nof course Visa, MasterCard  and they process high teens of global GDP. So they're pretty big. And then lastly you have the \\nmerchant  acquirers  and the way you can think about them is acting on the merchant's  behalf.\"),\n",
      " Document(page_content=\"And this is what Adyen is. [00:07:26]Zack: Now that we have a better appreciation  for all the players, take us through the flow of the transaction  as to \\nhow it actually manifests . [00:07:35]Michael: These transaction  will be separated  into the authorization  phase and the settlement  phase. So the process \\nstarts with the customer  who inputs their credentials . And this is where the data is first captured . So this can be tapping your \\nphone or card on a terminal  or having your card on file at the online checkout . Then the transaction  is relative to the merchant  \\nacquirer  who will perform certain risk checks. They then send that transaction  to the card networks  who then speak to the \\nissuing bank. They then validate the identity of the customer  and see if they have funds and will authorize  the payment . This is \\nthen sent back to the card network  who sends the authorization  to the merchant  acquirer, who then provides  the green light to \\nthe merchant . On the point of sale terminal , you will see approved  and you'll have your morning  coffee and receipt. The \\nsettlement  phase of the transaction  is not in real time and involves  the merchant  acquirer  and the issuing bank sharing \\ninformation  to allow for the transaction  to be on the customer's  bank statement . And then the merchant  acquirer  will deposit \\nthe funds into the merchants  bank account . I have simplified  this as there can be additional  participants  in the chain that can \\nall perform different  tasks. So for example , there are different  merchant  acquirer  categories , such as front end processes  sometimes  called pay facts, or \\naggregators , like Square or PayPal, and who effectively  do the authorization , but are laid on top of backend  processes  like first \\ndata who do the settlement  for them. There are sometimes  different  companies  who do risk management  and sometimes  \\ndifferent  companies  who act as a gateway . Then on the other side of the transaction , you have issuer processes  like TSYS and \\nMarqeta  who sit in front of the issuing bank. Now, if this all sounds a little complicated , don't worry because  it is. Adyen \\nwanted to move away from this complexity  by simplifying  it and having more control end to end. So they considered  all the \\nmerchant  acquiring  functionalities , so risk, the gateway , or the terminal  and the processing  and acquiring  all onto one single \\nplatform .\")]\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    base_url=\"http://localhost:11434\",\t\n",
    "    model=\"llama3:instruct\"\n",
    ")\n",
    "\n",
    "semantic_chunker = SemanticChunker(embeddings, breakpoint_threshold_type=\"percentile\")\n",
    "#\n",
    "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
    "\n",
    "pprint(semantic_chunks[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting entities from each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianyang/miniconda3/envs/local-llm/lib/python3.9/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_lg' (3.3.0) was trained with spaCy v3.3.0 and may not be 100% compatible with the current version (3.7.5). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jianyang/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/jianyang/miniconda3/envs/local-llm/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jianyang/miniconda3/envs/local-llm/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/jianyang/miniconda3/envs/local-llm/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jianyang/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Introduction', 'This', 'Zack Fuss', 'we', 'European  based pay business', 'Adyen', 'Adyen', 'Amsterdam', 'a group', 'payments', ' entrepreneurs', 'who', 'a business', 'the space', 'their chance', 'a modern solution', 'the patchwork', ' legacy system', 'that', 'the business', 'I', 'Michael Willar', 'a portfolio  manager', 'Stenham  Asset \\nManagement', 'Our discussion', \"Adyen's single platform  solution\", 'detail', 'the driving force', 'their track record', '\\nprofitable  growth', 'why payments', 'a winner', 'all market', 'this breakdown', 'Adyen', \"A Bird's Eye View\", 'Adyen', 'Payments', '00:02:42]Zack', 'we', 'Adyen', 'processing  business', 'its nearly $70 billion \\nUS market cap', 'it', 'a business', 'that', 'American  investors']\n",
      "Verbs: ['break', 'base', 'find', 'build', 'sell', 'start', 'build', 'displace', 'force', 'use', 'breakdown', 'join', 'cover', 'drive', 'take', 'enjoy', 'break']\n",
      "Zack Fuss PERSON\n",
      "today DATE\n",
      "European NORP\n",
      "Amsterdam GPE\n",
      "2006 DATE\n",
      "Michael Willar PERSON\n",
      "Stenham ORG\n",
      "Adyen ORG\n",
      "today DATE\n",
      "nearly $70 billion MONEY\n",
      "American ORG\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English tokenizer, POS tagger, parser, NER and word vectors\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Process whole documents\n",
    "text = semantic_chunks[0].page_content\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Use LLM to recursively extract entities from each chunk\n",
    "llm = Ollama(model=\"llama3:instruct\", temperature=0, base_url=\"http://localhost:11434\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "entities_extraction_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    Extract all entities from the following text. \n",
    "    Present your answers in the format ie \"ENTITY_1;ENTITY_2;ENTITY_3\"\n",
    "    DO NOT give any preamble or explanation.\n",
    "    Some example entities: 'john;apple;new_york'\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Text: {text}\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"text\"],\n",
    ")\n",
    "\n",
    "entities_extraction_pipeline = entities_extraction_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_extraction_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "    Extract all relationships between the given entities from the given text.\n",
    "    Present your answers in the format ie \"ENTITY_1|RELATIONSHIP_1|ENTITY_2;ENTITY_2|RELATIONSHIP_2|ENTITY_3\"\n",
    "    DO NOT give any preamble or explanation.\n",
    "    Some example relationships:\n",
    "    'sarah_johnson|HAS_SKILL|machine_learning;sarah_johnson|HAS_SKILL|data_analytics'\n",
    "\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Text: {text}\n",
    "    Identified entities: {entities}\n",
    "    ALready identified relationship terms: {existing_rs_terms}\n",
    "    IMPORTANT: You are NOT to extract any entities. Use the given entities to extract relationships.\n",
    "    IMPORTANT: DO NOT USE ANY ENTITIES THAT ARE NOT IN THE \"Identified entities\" LIST.\n",
    "    <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"text\", \"entities\", \"existing_rs_terms\"],\n",
    ")\n",
    "\n",
    "rs_extraction_pipeline = rs_extraction_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities_from_text(text):\n",
    "    entities = entities_extraction_pipeline.invoke({\"text\": text})\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rs_from_text_and_entities(text, entities, existing_rs_terms):\n",
    "    rs_from_chunk = rs_extraction_pipeline.invoke({\"text\": text, \"entities\": entities, \"existing_rs_terms\": existing_rs_terms})\n",
    "\n",
    "    return rs_from_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: There might stil be some entities in the relationships that are not in the entities list. We will replace the entities set with the entities extracted from the relationships. \n",
    "def get_entities_and_rs_from_chunks(chunks: list):\n",
    "    entities = set()\n",
    "    rs = set()\n",
    "    rs_terms = set()\n",
    "\n",
    "    for chunk in chunks:\n",
    "        chunk_text = chunk.page_content\n",
    "\n",
    "        # Extract ENTITIES from chunk\n",
    "        unformatted_entities = get_entities_from_text(chunk_text)\n",
    "        # Remove leading and trailing whitespaces and Format entities\n",
    "        # curr_entities_list = [entity.strip() for entity in unformatted_entities.split(\";\")]\n",
    "        curr_entities_list = [re.sub(r'\\W+', '', entity.strip().replace(\" \", \"_\")) for entity in unformatted_entities.split(\";\")]\n",
    "        # print(\"Pre-cleaned entities: \", curr_entities_list)\n",
    "        # Only retain alphanumeric entities\n",
    "        cleaned_entities_list = [''.join(re.findall(r'\\w+', entity)).lower() for entity in curr_entities_list]\n",
    "        # print(\"Cleaned entities: \", cleaned_entities_list)\n",
    "        # Update entities set\n",
    "        entities.update(cleaned_entities_list)\n",
    "\n",
    "        # Extract RELATIONSHIPS from chunk\n",
    "        unformatted_rs = get_rs_from_text_and_entities(chunk_text, cleaned_entities_list, rs_terms)\n",
    "        # Remove leading and trailing whitespaces and Format relationships\n",
    "        curr_rs_list = [rs.strip() for rs in unformatted_rs.split(\";\")]\n",
    "        # Update relationships set\n",
    "        rs.update(curr_rs_list)\n",
    "\n",
    "        # Extract RELATIONSHIP TERMS from chunk\n",
    "        curr_rs_terms = []\n",
    "        for relationship in curr_rs_list:\n",
    "            curr_rs_terms.append(relationship.split(\"|\")[1].strip())\n",
    "        # Update relationship terms set\n",
    "        rs_terms.update(curr_rs_terms)\n",
    "\n",
    "    return (entities, rs, rs_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, relationships, _  = get_entities_and_rs_from_chunks(semantic_chunks[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Micheal_willar|WORKS_AT|Stenham_Asset_Management', 'Adyen|FOUND_IN|Amsterdam', 'adyen|OPERATES_IN|apac', 'adyen|HAS_BUSINESS|nike', 'adyen|IS_HEADQUARTERED_IN|amsterdam', 'adyen|OPERATES_IN|latin_america', 'adyen|HAS_BUSINESS|overmatch', 'adyen|OPERATES_IN|europe', 'adyen|OPERATES_IN|us', 'Zack_fuss|IS_JOINED_BY|Micheal_willar', 'adyen|HAS_BUSINESS|mcdonalds', 'adyen|HAS_BUSINESS|microsoft'}\n"
     ]
    }
   ],
   "source": [
    "print(relationships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_id micheal_willar | rs_type WORKS_AT | tgt_id stenham_asset_management\n",
      "\n",
      "src_id adyen | rs_type FOUND_IN | tgt_id amsterdam\n",
      "\n",
      "src_id adyen | rs_type OPERATES_IN | tgt_id apac\n",
      "\n",
      "src_id adyen | rs_type HAS_BUSINESS | tgt_id nike\n",
      "\n",
      "src_id adyen | rs_type IS_HEADQUARTERED_IN | tgt_id amsterdam\n",
      "\n",
      "src_id adyen | rs_type OPERATES_IN | tgt_id latin_america\n",
      "\n",
      "src_id adyen | rs_type HAS_BUSINESS | tgt_id overmatch\n",
      "\n",
      "src_id adyen | rs_type OPERATES_IN | tgt_id europe\n",
      "\n",
      "src_id adyen | rs_type OPERATES_IN | tgt_id us\n",
      "\n",
      "src_id zack_fuss | rs_type IS_JOINED_BY | tgt_id micheal_willar\n",
      "\n",
      "src_id adyen | rs_type HAS_BUSINESS | tgt_id mcdonalds\n",
      "\n",
      "src_id adyen | rs_type HAS_BUSINESS | tgt_id microsoft\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r_statements = []\n",
    "\n",
    "for rs in relationships:\n",
    "    src_id, rs_type, tgt_id = rs.split(\"|\")\n",
    "    src_id = src_id.replace(\"-\", \"\").lower()\n",
    "    print(\"src_id\", src_id, end = \" | \") \n",
    "    print(\"rs_type\", rs_type, end = \" | \")\n",
    "    tgt_id = tgt_id.replace(\"-\", \"\").lower()\n",
    "    print(\"tgt_id\", tgt_id)\n",
    "    print()\n",
    "            \n",
    "    cypher = f'MERGE (a:Recursive_Test {{id: \"{src_id}\"}}) MERGE (b:Recursive_Test {{id: \"{tgt_id}\"}}) MERGE (a)-[:{rs_type}]->(b)'\n",
    "    r_statements.append(cypher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "url = \"bolt://localhost:7687\"\n",
    "username = \"neo4j\"\n",
    "password = \"password\"\n",
    "gds = GraphDatabase.driver(url, auth=(username, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing cypher statement 1 of 12\n",
      "Executing cypher statement 2 of 12\n",
      "Executing cypher statement 3 of 12\n",
      "Executing cypher statement 4 of 12\n",
      "Executing cypher statement 5 of 12\n",
      "Executing cypher statement 6 of 12\n",
      "Executing cypher statement 7 of 12\n",
      "Executing cypher statement 8 of 12\n",
      "Executing cypher statement 9 of 12\n",
      "Executing cypher statement 10 of 12\n",
      "Executing cypher statement 11 of 12\n",
      "Executing cypher statement 12 of 12\n"
     ]
    }
   ],
   "source": [
    "# Generate and execute cypher statements\n",
    "cypher_statements = r_statements\n",
    "for i, stmt in enumerate(cypher_statements):\n",
    "    print(f\"Executing cypher statement {i+1} of {len(cypher_statements)}\")\n",
    "    try:\n",
    "        gds.execute_query(stmt)\n",
    "    except Exception as e:\n",
    "        with open(\"failed_statements.txt\", \"w\") as f:\n",
    "            f.write(f\"{stmt} - Exception: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received notification from DBMS server: {severity: WARNING} {code: Neo.ClientNotification.Statement.FeatureDeprecationWarning} {category: DEPRECATION} {title: This feature is deprecated and will be removed in future versions.} {description: The query used a deprecated procedure. ('db.create.setVectorProperty' has been replaced by 'db.create.setNodeVectorProperty')} {position: line: 1, column: 76, offset: 75} for query: \"UNWIND $data AS row MATCH (n:`Recursive_Test`) WHERE elementId(n) = row.id CALL db.create.setVectorProperty(n, 'embedding', row.embedding) YIELD node RETURN count(*)\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Neo4jVector\n",
    "\n",
    "existing_graph = Neo4jVector.from_existing_graph(\n",
    "    embedding=embeddings,\n",
    "    url=url,\n",
    "    username=username,\n",
    "    password=password,\n",
    "    node_label=\"Recursive_Test\",\n",
    "    text_node_properties=[\"id\"],\n",
    "    embedding_node_property=\"embedding\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](images/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node = existing_graph.similarity_search(\"the company that process payments\", top_k=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root entity:  adyen\n"
     ]
    }
   ],
   "source": [
    "root_entity = root_node.page_content.strip()[3:]\n",
    "print(\"Root entity:\", root_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all immediate entities and relationships from node with id \"adyen\"\n",
    "with gds.session() as session:\n",
    "    curr_knowledge = []\n",
    "    result = session.run(\"MATCH (a:Recursive_Test {id: 'adyen'})-[r]-(b) RETURN a, r, b\")\n",
    "    for record in result:\n",
    "        curr_knowledge.append(f'{record[\"a\"][\"id\"]}-{record[\"r\"].type}-{record[\"b\"][\"id\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adyen-HAS_BUSINESS-microsoft',\n",
       " 'adyen-HAS_BUSINESS-mcdonalds',\n",
       " 'adyen-OPERATES_IN-us',\n",
       " 'adyen-OPERATES_IN-europe',\n",
       " 'adyen-HAS_BUSINESS-overmatch',\n",
       " 'adyen-OPERATES_IN-latin_america',\n",
       " 'adyen-IS_HEADQUARTERED_IN-amsterdam',\n",
       " 'adyen-HAS_BUSINESS-nike',\n",
       " 'adyen-OPERATES_IN-apac',\n",
       " 'adyen-FOUND_IN-amsterdam']"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"WHAT OTHER COMPANIES are located in the same place as adyen's headquarters?\"\n",
    "# query = \"Where is Adyen headquartered?\"\n",
    "query = \"What does Adyen do?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = llm.invoke(f\"\"\"\n",
    "    You are trying to see if a given question can be answered with the current knowledge and answer it if possible.\n",
    "    You are given this knowledge: {curr_knowledge}.\n",
    "    The knowledge is formatted as \"currentNode-HAS_A_RELATIONSHIP-possibleNextNode\".\n",
    "    You are currently exploring all the relationships of the \"currentNode\" in the knowledge.\n",
    "    You are asked the question: \"{query}\".\n",
    "    Assume that you are traversing a knowledge graph and the knowledge given earlier is NOT all the information you have.\n",
    "    If this current node doesn't help us answer the question, we want to determine which node to explore next. \n",
    "\n",
    "    The task:\n",
    "    If you can answer the question with the current knowledge, provide the answer and explain how your answer answers the question.\n",
    "    If you cannot answer the question with the current knowledge, either say \"give up\" OR specify the relationship that will help you answer the question.\n",
    "    \n",
    "    In addition, when you cannot answer the question, give the relationship that you want to explore next in a new line without any preamble or explanation.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll start by exploring the relationships of Adyen.\n",
      "\n",
      "The current knowledge is: ['adyen-HAS_BUSINESS-microsoft', 'adyen-HAS_BUSINESS-mcdonalds', 'adyen-OPERATES_IN-us', 'adyen-OPERATES_IN-europe', 'adyen-HAS_BUSINESS-overmatch', 'adyen-OPERATES_IN-latin_america', 'adyen-IS_HEADQUARTERED_IN-amsterdam', 'adyen-HAS_BUSINESS-nike', 'adyen-OPERATES_IN-apac', 'adyen-FOUND_IN-amsterdam']\n",
      "\n",
      "Since Adyen is the current node, I'll look at its relationships.\n",
      "\n",
      "The question is: \"What does Adyen do?\"\n",
      "\n",
      "I can answer this question with the current knowledge. The answer is that Adyen has businesses in various companies such as Microsoft, McDonald's, Overmatch, and Nike, which suggests that it provides some kind of services or solutions to these companies. Additionally, its presence in different regions like Europe, Latin America, APAC, and US implies that it operates globally.\n",
      "\n",
      "So, the answer is: Adyen has businesses in various companies and operates globally.\n",
      "\n",
      "adyen-OPERATES_IN-apac\n"
     ]
    }
   ],
   "source": [
    "print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = (llm.invoke(f\"You are a critic trying to detect gaps in someone's logic in their statement. \\\n",
    "                       The question posed to the person is: ({query}). The knowledge they and you have is limited to: ({curr_knowledge}). \\\n",
    "                        Given the statement below, if the statement is logically sound, respond with 'good'. \\\n",
    "                        If the statement is not logically sound or doesnt fully answer the question and requires further exploration of extensions to the knowledge, \\\n",
    "                            explain why. Suggest DIRECTLY how to correct the answer. Statement: {statement}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not quite \"good\" yet!\n",
      "\n",
      "The statement attempts to answer the question \"What does Adyen do?\" by highlighting its relationships with other entities. However, it doesn't fully address the question. The statement only mentions that Adyen has businesses in various companies and operates globally, but it doesn't explicitly state what kind of services or solutions Adyen provides.\n",
      "\n",
      "To make the answer more comprehensive and logically sound, I would suggest adding a direct connection between Adyen's relationships with these companies and its services or solutions. For example:\n",
      "\n",
      "\"I can answer this question with the current knowledge. The answer is that Adyen has businesses in various companies such as Microsoft, McDonald's, Overmatch, and Nike, which suggests that it provides payment processing or e-commerce solutions to these companies. Additionally, its presence in different regions like Europe, Latin America, APAC, and US implies that it operates globally.\"\n",
      "\n",
      "By explicitly stating the services or solutions Adyen provides, the answer becomes more complete and logically sound.\n"
     ]
    }
   ],
   "source": [
    "print(feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "reattempted_statement = llm.invoke(f\"\"\"\n",
    "    You were given the task as follows:\n",
    "        ***TASK START***\n",
    "        You are given this knowledge: {curr_knowledge}.\n",
    "        The knowledge is formatted as \"currentNode-HAS_A_RELATIONSHIP-possibleNextNode\".\n",
    "        You are currently exploring all the relationships of the \"currentNode\" in the knowledge.\n",
    "        You are asked the question: \"{query}\".\n",
    "        Assume that you are traversing a knowledge graph and the knowledge given earlier is NOT all the information you have.\n",
    "        If this current node doesn't help us answer the question, we want to determine which node to explore next. \n",
    "\n",
    "        The task:\n",
    "        If you can answer the question with the current knowledge, provide the answer.\n",
    "        If you cannot answer the question with the current knowledge, either say \"give up\" OR specify the relationship that will help you answer the question.\n",
    "        \n",
    "        ***TASK END***\n",
    "\n",
    "    You gave an answer: {statement}.\n",
    "    BUT you were given the feedback: {feedback}.\n",
    "\n",
    "    Correct your answer based on the feedback given. \n",
    "\n",
    "    Format the LAST LINE of your answer as follows:\n",
    "    If you CAN answer the question, give in the format \"ANSWER: <your answer>\".\n",
    "    If you CANNOT answer the question, give in the format \"RELATIONSHIP: <relationship to explore next> or GIVE UP\".\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll start by exploring the relationships of Adyen.\n",
      "\n",
      "The current knowledge is: ['adyen-HAS_BUSINESS-microsoft', 'adyen-HAS_BUSINESS-mcdonalds', 'adyen-OPERATES_IN-us', 'adyen-OPERATES_IN-europe', 'adyen-HAS_BUSINESS-overmatch', 'adyen-OPERATES_IN-latin_america', 'adyen-IS_HEADQUARTERED_IN-amsterdam', 'adyen-HAS_BUSINESS-nike', 'adyen-OPERATES_IN-apac', 'adyen-FOUND_IN-amsterdam']\n",
      "\n",
      "Since Adyen is the current node, I'll look at its relationships.\n",
      "\n",
      "The question is: \"What does Adyen do?\"\n",
      "\n",
      "I can answer this question with the current knowledge. The answer is that Adyen has businesses in various companies such as Microsoft, McDonald's, Overmatch, and Nike, which suggests that it provides some kind of services or solutions to these companies. Additionally, its presence in different regions like Europe, Latin America, APAC, and US implies that it operates globally.\n",
      "\n",
      "I can provide more context by explicitly stating the services or solutions Adyen provides.\n",
      "\n",
      "ANSWER: Adyen has businesses in various companies such as Microsoft, McDonald's, Overmatch, and Nike, which suggests that it provides payment processing or e-commerce solutions to these companies. Additionally, its presence in different regions like Europe, Latin America, APAC, and US implies that it operates globally.\n"
     ]
    }
   ],
   "source": [
    "print(reattempted_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: adyen has businesses in various companies, such as microsoft, mcdonald's, overmatch, and nike.\n"
     ]
    }
   ],
   "source": [
    "# Explore the next node based on the feedback\n",
    "agent_response = reattempted_statement.split(\"\\n\")[-1].lower().strip()\n",
    "header, agent_verdict = agent_response.split(\":\")[0].strip(), agent_response.split(\":\")[1].strip()\n",
    "\n",
    "if header == \"relationship\":\n",
    "    if agent_verdict == \"give up\":\n",
    "        print(\"Agent has given up\")\n",
    "    else:\n",
    "        print(\"Agent wants to explore the relationship:\", agent_verdict)\n",
    "        entity_to_explore = agent_verdict.split(\"-\")[-1]\n",
    "        print(\"Entity to explore:\", entity_to_explore)\n",
    "        # Extract all immediate entities and relationships from node with id \"adyen\"\n",
    "        with gds.session() as session:\n",
    "            curr_knowledge = []\n",
    "            query_statement = \"MATCH (a:Recursive_Test {id: '\" + entity_to_explore + \"'})-[r]-(b) RETURN a, r, b\"\n",
    "            result = session.run(query_statement)\n",
    "            for record in result:\n",
    "                curr_knowledge.append(f'{record[\"a\"][\"id\"]}-{record[\"r\"].type}-{record[\"b\"][\"id\"]}')\n",
    "\n",
    "        print(curr_knowledge)\n",
    "else:\n",
    "    print(\"Answer:\", agent_verdict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!! Then the idea is that we recurse back from here, slowly exploring the graph and taking notes along the way, terminating ONLY if we arrive back at the same starting node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [!IMPORTANT] The use of an adversarial agent to check the output helps tremendously in ensuring accuracy of the answer and adherence to the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Close the driver\n",
    "gds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
